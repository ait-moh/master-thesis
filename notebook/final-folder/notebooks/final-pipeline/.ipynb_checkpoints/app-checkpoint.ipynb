{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d17db-3148-4c28-b740-5feaff2164e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\apps\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 26 features, but MinMaxScaler is expecting 25 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m denorm_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(denorm, columns\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# === 8. Re-normalize for Detection Model ===\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m scaled_for_detection \u001b[38;5;241m=\u001b[39m \u001b[43mdetection_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenorm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m scaled_for_detection \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(scaled_for_detection, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# === 9. Anomaly Detection ===\u001b[39;00m\n",
      "File \u001b[1;32mE:\\apps\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mE:\\apps\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:532\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    528\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    530\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 532\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    543\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[1;32mE:\\apps\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mE:\\apps\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 26 features, but MinMaxScaler is expecting 25 features as input."
     ]
    }
   ],
   "source": [
    "# === 0. Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# === 1. Load Models ===\n",
    "forecast_model = tf.keras.models.load_model(\"best_lstm_seq2seq_forecaster.h5\")\n",
    "detection_model = tf.keras.models.load_model(\"best_transformer_autoencoder.h5\")\n",
    "\n",
    "# === 2. Load Scalers ===\n",
    "forecast_scaler = joblib.load(\"forecast_scaler.pkl\")\n",
    "detection_scaler = joblib.load(\"detection_scaler.pkl\")\n",
    "\n",
    "# === 3. Load Full Test Set ===\n",
    "file_path = \"../../data/test_sequences/full_test.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "if 'DateTime' in df.columns:\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# === 4. Save and Drop Labels ===\n",
    "INPUT_STEPS = 10\n",
    "FORECAST_STEPS = 10\n",
    "\n",
    "if 'labels' in df.columns:\n",
    "    ground_truth_labels = df['labels'].values[INPUT_STEPS + FORECAST_STEPS - 1:]\n",
    "    df = df.drop(columns=['labels'])\n",
    "else:\n",
    "    ground_truth_labels = None\n",
    "\n",
    "# === 5. Normalize for Forecast ===\n",
    "scaled_forecast = forecast_scaler.transform(df.values)\n",
    "df_scaled_forecast = pd.DataFrame(scaled_forecast, index=df.index, columns=df.columns)\n",
    "\n",
    "# === 6. Create Forecasting Sequences ===\n",
    "def create_forecast_sequences(data, input_steps, forecast_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_steps - forecast_steps + 1):\n",
    "        X.append(data[i:i+input_steps])\n",
    "        y.append(data[i+input_steps:i+input_steps+forecast_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_forecast_seq, y_true_forecast_seq = create_forecast_sequences(df_scaled_forecast.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "\n",
    "# === 7. Forecast ===\n",
    "y_pred = forecast_model.predict(X_forecast_seq)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === 8. Evaluate Forecasting ===\n",
    "y_pred_flat = y_pred.reshape(-1, y_pred.shape[2])\n",
    "y_true_flat = y_true_forecast_seq.reshape(-1, y_true_forecast_seq.shape[2])\n",
    "\n",
    "# ⚠️ Calculate MSE and MAE in normalized space (same as training)\n",
    "forecast_mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
    "forecast_mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "\n",
    "print(f\"📊 Forecast Evaluation (on normalized data):\")\n",
    "print(f\"   - MSE: {forecast_mse:.5f}\")\n",
    "print(f\"   - MAE: {forecast_mae:.5f}\")\n",
    "\n",
    "# === 9. De-normalize Forecast Output for Detection Phase ===\n",
    "y_pred_denorm = forecast_scaler.inverse_transform(y_pred_flat)\n",
    "\n",
    "\n",
    "# === 9. Detection Phase on Forecast Output ===\n",
    "scaled_for_detection = detection_scaler.transform(y_pred_denorm)\n",
    "\n",
    "X_detect_seq = np.array([\n",
    "    scaled_for_detection[i:i+INPUT_STEPS]\n",
    "    for i in range(len(scaled_for_detection) - INPUT_STEPS)\n",
    "])\n",
    "\n",
    "reconstructed = detection_model.predict(X_detect_seq)\n",
    "reconstruction_errors = np.mean((X_detect_seq - reconstructed) ** 2, axis=(1, 2))\n",
    "\n",
    "# === 10. Thresholding ===\n",
    "MANUAL_PERCENTILE = 90\n",
    "threshold = np.percentile(reconstruction_errors, MANUAL_PERCENTILE)\n",
    "anomaly_flags = (reconstruction_errors > threshold).astype(int)\n",
    "\n",
    "print(f\"\\n🚨 Anomaly Detection Summary:\")\n",
    "print(f\"   - Threshold: {threshold:.6f}\")\n",
    "print(f\"   - Detected Anomalies: {anomaly_flags.sum()} / {len(anomaly_flags)}\")\n",
    "\n",
    "# === 11. Evaluate Detection (if ground truth exists) ===\n",
    "if ground_truth_labels is not None and len(ground_truth_labels) == len(anomaly_flags):\n",
    "    precision = precision_score(ground_truth_labels, anomaly_flags)\n",
    "    recall = recall_score(ground_truth_labels, anomaly_flags)\n",
    "    f1 = f1_score(ground_truth_labels, anomaly_flags)\n",
    "    print(\"\\n🎯 Detection Evaluation:\")\n",
    "    print(f\"   - Precision: {precision:.4f}\")\n",
    "    print(f\"   - Recall:    {recall:.4f}\")\n",
    "    print(f\"   - F1-score:  {f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Ground truth labels not available or length mismatch for detection evaluation.\")\n",
    "\n",
    "# === 12. Optional Plot ===\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(reconstruction_errors, label=\"Reconstruction Error\")\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label=\"Threshold\")\n",
    "plt.title(\"Reconstruction Error over Sliding Windows\")\n",
    "plt.xlabel(\"Sliding Window Index\")\n",
    "plt.ylabel(\"MSE Error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24020800-cb8c-4d52-bb59-efe3275af206",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
