{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17614507-c3a5-4275-8f5a-bfcd76c4fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 0. IMPORTS\n",
    "# ========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, LSTM, RepeatVector, TimeDistributed,\n",
    "                                    MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D,\n",
    "                                    Conv1D)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "# ========================\n",
    "# 1. CONFIGURATION\n",
    "# ========================\n",
    "INPUT_STEPS = 10\n",
    "FORECAST_STEPS = 10\n",
    "TEST_RATIO = 0.2\n",
    "# Tuning parameters\n",
    "EPOCHS_LIST = [20]\n",
    "BATCH_SIZES = [128]\n",
    "WINDOW_SIZE_SIMULATION = 10  # 6h window\n",
    "# Transformer parameters for anomaly detection\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 256\n",
    "DROPOUT_RATE = 0.1\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "# ========================\n",
    "# 2. DEVICE SETUP\n",
    "# ========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(\"✅ GPU is available and will be used.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected, running on CPU.\")\n",
    "# ========================\n",
    "# 3. LOAD AND PREPROCESS DATA\n",
    "# ========================\n",
    "file_path = '../../data/cleaned_labeled_dataset.csv'\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'], errors='coerce')\n",
    "df.set_index('DateTime', inplace=True)\n",
    "\n",
    "file_path2= '../../data/preprocessed_data.csv'\n",
    "df_forecast = pd.read_csv(file_path, delimiter=',')\n",
    "df_forecast['DateTime'] = pd.to_datetime(df_forecast['DateTime'], errors='coerce')\n",
    "df_forecast.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Extract labels column if it exists and drop it from the dataset\n",
    "if 'labels' in df.columns:\n",
    "    print(\"✅ Found 'labels' column. Extracting for later evaluation.\")\n",
    "    labels_series = df['labels'].copy()\n",
    "    df = df.drop(columns=['labels'])\n",
    "else:\n",
    "    print(\"⚠️ No 'labels' column found. Will assume all samples are normal.\")\n",
    "    labels_series = pd.Series(np.zeros(len(df)))\n",
    "\n",
    "# Extract labels column if it exists and drop it from the dataset\n",
    "if 'labels' in df_forecast.columns:\n",
    "    print(\"✅ Found 'labels' column. Extracting for later evaluation.\")\n",
    "    labels_df_forecast = df_forecast['labels'].copy()\n",
    "    df_forecast = df_forecast.drop(columns=['labels'])\n",
    "else:\n",
    "    print(\"⚠️ No 'labels' column found. Will assume all samples are normal.\")\n",
    "    df_forecast = pd.Series(np.zeros(len(df_forecast)))\n",
    "\n",
    "    \n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df.values)\n",
    "df_scaled = pd.DataFrame(scaled_data, index=df.index, columns=df.columns).astype(np.float32)\n",
    "print(f\"✅ Scaled dataset shape: {df_scaled.shape}\")\n",
    "\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data_forecast = scaler.fit_transform(df_forecast.values)\n",
    "scaled_data_forecast2 = pd.DataFrame(scaled_data_forecast, index=df_forecast.index, columns=df_forecast.columns).astype(np.float32)\n",
    "print(f\"✅ Scaled dataset shape: {scaled_data_forecast2.shape}\")\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 4. SEQUENTIAL TRAIN/TEST SPLIT\n",
    "# ========================\n",
    "split_idx = int((1 - TEST_RATIO) * len(df_scaled))\n",
    "train_data_detection = df_scaled.iloc[:split_idx]\n",
    "train_data = scaled_data_forecast2.iloc[:split_idx]\n",
    "test_data = scaled_data_forecast2.iloc[split_idx:]\n",
    "# Split labels in the same way\n",
    "train_labels = labels_df_forecast.iloc[:split_idx]\n",
    "test_labels = labels_df_forecast.iloc[split_idx:]\n",
    "print(f\"✅ Training samples: {len(train_data)}, Testing samples: {len(test_data)}\")\n",
    "# ========================\n",
    "# 5. CREATE SEQUENCES\n",
    "# ========================\n",
    "def create_sequences(data, input_steps, forecast_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_steps - forecast_steps):\n",
    "        X.append(data[i:i+input_steps])\n",
    "        y.append(data[i+input_steps:i+input_steps+forecast_steps])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "# Create sequence labels (a sequence is anomalous if any point in it is anomalous)\n",
    "def create_sequence_labels(labels, input_steps, forecast_steps):\n",
    "    \"\"\"\n",
    "    Create sequence-level labels from point-level labels.\n",
    "    A sequence is considered anomalous (1) if any point in its forecast window is anomalous.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array of point-level binary labels (0=normal, >0=anomaly)\n",
    "        input_steps: Number of input steps (not used for labeling)\n",
    "        forecast_steps: Number of forecast steps\n",
    "        \n",
    "    Returns:\n",
    "        Array of sequence-level binary labels\n",
    "    \"\"\"\n",
    "    seq_labels = []\n",
    "    for i in range(len(labels) - input_steps - forecast_steps):\n",
    "        # If any point in the forecast window is anomalous, mark the sequence as anomalous\n",
    "        forecast_window_labels = labels[i+input_steps:i+input_steps+forecast_steps]\n",
    "        # Convert any non-zero value to anomaly (1)\n",
    "        is_anomalous = np.any(np.array(forecast_window_labels) > 0)\n",
    "        seq_labels.append(1 if is_anomalous else 0)\n",
    "    return np.array(seq_labels, dtype=np.int32)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(train_data.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(test_data.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "\n",
    "# Create labels for test sequences - used for anomaly evaluation\n",
    "test_seq_labels = create_sequence_labels(test_labels.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "print(f\"✅ Training sequences: {X_train_seq.shape}, Testing sequences: {X_test_seq.shape}\")\n",
    "print(f\"✅ Test sequence labels shape: {test_seq_labels.shape}, with {np.sum(test_seq_labels)} anomalous sequences\")\n",
    "\n",
    "# ========================\n",
    "# 6. BUILD LSTM SEQ2SEQ MODEL\n",
    "# ========================\n",
    "def build_lstm_seq2seq(input_steps, forecast_steps, input_dim, units=128):\n",
    "    inputs = Input(shape=(input_steps, input_dim))\n",
    "    encoded = LSTM(units)(inputs)\n",
    "    repeated = RepeatVector(forecast_steps)(encoded)\n",
    "    decoded = LSTM(units, return_sequences=True)(repeated)\n",
    "    outputs = TimeDistributed(Dense(input_dim))(decoded)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# ========================\n",
    "# 7. TRAINING + TUNING\n",
    "# ========================\n",
    "best_val_rmse = np.inf\n",
    "best_model = None\n",
    "history_records = []\n",
    "for epochs in EPOCHS_LIST:\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n🔵 Training LSTM Seq2Seq with epochs={epochs}, batch_size={batch_size}\")\n",
    "\n",
    "        model = build_lstm_seq2seq(INPUT_STEPS, FORECAST_STEPS, X_train_seq.shape[2])\n",
    "        es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        history = model.fit(X_train_seq, y_train_seq,\n",
    "                            validation_split=0.1,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[es],\n",
    "                            verbose=1,\n",
    "                            shuffle=False)\n",
    "\n",
    "        val_preds = model.predict(X_test_seq, batch_size=batch_size)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_test_seq.reshape(-1), val_preds.reshape(-1)))\n",
    "        val_mae = mean_absolute_error(y_test_seq.reshape(-1), val_preds.reshape(-1))\n",
    "        print(f\"✅ Validation RMSE: {val_rmse:.5f}, MAE: {val_mae:.5f}\")\n",
    "        history_records.append({\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"val_mae\": val_mae\n",
    "        })\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_model = model\n",
    "# Save tuning history\n",
    "history_df = pd.DataFrame(history_records)\n",
    "history_df.to_csv(\"lstm_seq2seq_tuning_history.csv\", index=False)\n",
    "print(\"\\n📋 Tuning Results Summary:\")\n",
    "print(history_df)\n",
    "# Save best model\n",
    "best_model.save(\"best_lstm_seq2seq_forecaster.keras\")\n",
    "print(\"\\n✅ Best LSTM Seq2Seq model saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b3bec-20d5-4d08-b296-a9ff262d7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 8. BUILD AND TRAIN TRANSFORMER AUTOENCODER FOR ANOMALY DETECTION\n",
    "# ========================\n",
    "def create_ae_sequences(data, seq_len):\n",
    "    return np.array([data[i:i+seq_len] for i in range(len(data) - seq_len)], dtype=np.float32)\n",
    "X_ae_train = create_ae_sequences(train_data_detection.values, FORECAST_STEPS)\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "\n",
    "    # Feed-forward network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "\n",
    "    # Add & Norm\n",
    "    return LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "def build_transformer_autoencoder(input_steps, input_dim, embed_dim=128, num_heads=4, ff_dim=256, dropout_rate=0.1):\n",
    "    inputs = Input(shape=(input_steps, input_dim))\n",
    "\n",
    "    # Initial projection to embed_dim\n",
    "    x = Conv1D(filters=embed_dim, kernel_size=1, activation='relu')(inputs)\n",
    "\n",
    "    # Encoder: Transformer blocks\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "    # Bottleneck\n",
    "    encoded = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Decoder: Expand to sequence\n",
    "    x = RepeatVector(input_steps)(encoded)\n",
    "\n",
    "    # Decoder: Transformer blocks\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "    # Output projection back to original dimensions\n",
    "    outputs = TimeDistributed(Dense(input_dim))(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "# Build and train the transformer autoencoder for anomaly detection\n",
    "transformer_ae = build_transformer_autoencoder(\n",
    "    FORECAST_STEPS, \n",
    "    X_ae_train.shape[2],\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "# Train Transformer AE\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "transformer_ae.fit(\n",
    "    X_ae_train, \n",
    "    X_ae_train, \n",
    "    validation_split=0.1, \n",
    "    epochs=20, \n",
    "    batch_size=128, \n",
    "    callbacks=[es], \n",
    "    verbose=1\n",
    ")\n",
    "transformer_ae.save(\"best_transformer_autoencoder.h5\")\n",
    "print(\"\\n✅ Transformer Autoencoder trained and saved.\")\n",
    "\n",
    "# Use the last 3000 points of the test data for simulation\n",
    "test_data_tail = test_data.tail(3000).reset_index(drop=True)\n",
    "test_labels_tail = test_labels.tail(3000).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef119217-ab29-4e55-ad1b-29f9c3c5093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use the last 3000 points of the test data for simulation\n",
    "test_data_tail = test_data.tail(3000).reset_index(drop=True)\n",
    "test_labels_tail = test_labels.tail(3000).reset_index(drop=True)\n",
    "\n",
    "# ========================\n",
    "# 9. REAL-TIME SIMULATION WITH MANUAL THRESHOLD CONTROL\n",
    "# ========================\n",
    "# Import necessary libraries for ROC curve analysis\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Function to find optimal threshold using ROC curve\n",
    "def find_optimal_threshold_roc(y_true, reconstruction_errors):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold using ROC curve analysis.\n",
    "    Returns the threshold that maximizes Youden's J statistic (TPR - FPR)\n",
    "    \"\"\"\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, reconstruction_errors)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Find the threshold that maximizes TPR - FPR (Youden's J statistic)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    print(f\"✅ ROC analysis - Optimal threshold: {optimal_threshold:.5f}\")\n",
    "    print(f\"✅ ROC analysis - AUC: {roc_auc:.5f}\")\n",
    "    print(f\"✅ ROC analysis - At optimal threshold: TPR={tpr[optimal_idx]:.5f}, FPR={fpr[optimal_idx]:.5f}\")\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "# Function to detect anomalies with manual threshold\n",
    "def detect_anomalies(reconstruction_errors, threshold=None, percentile=None, y_true=None):\n",
    "    \"\"\"\n",
    "    Detect anomalies using either:\n",
    "    1. Manual threshold value\n",
    "    2. Percentile-based threshold\n",
    "    3. ROC optimal threshold (if y_true is provided)\n",
    "    \n",
    "    Returns anomaly flags and the threshold used\n",
    "    \"\"\"\n",
    "    # Get reconstruction errors as a flat array\n",
    "    flat_errors = np.concatenate(reconstruction_errors) if isinstance(reconstruction_errors, list) else reconstruction_errors\n",
    "    \n",
    "    # Determine the threshold to use\n",
    "    if threshold is not None:\n",
    "        # Use manually specified threshold\n",
    "        used_threshold = threshold\n",
    "        print(f\"Using manually specified threshold: {used_threshold:.5f}\")\n",
    "    elif percentile is not None:\n",
    "        # Calculate percentile-based threshold\n",
    "        used_threshold = np.percentile(flat_errors, percentile)\n",
    "        print(f\"Using {percentile}th percentile threshold: {used_threshold:.5f}\")\n",
    "    elif y_true is not None:\n",
    "        # Find optimal threshold using ROC curve\n",
    "        used_threshold = find_optimal_threshold_roc(y_true, flat_errors)\n",
    "        print(f\"Using ROC optimal threshold: {used_threshold:.5f}\")\n",
    "    else:\n",
    "        # Default: use 99.9th percentile\n",
    "        used_threshold = np.percentile(flat_errors, 99.9)\n",
    "        print(f\"Using default 99.9th percentile threshold: {used_threshold:.5f}\")\n",
    "    \n",
    "    # Apply threshold to detect anomalies\n",
    "    if isinstance(reconstruction_errors, list):\n",
    "        anomaly_flags = [errors > used_threshold for errors in reconstruction_errors]\n",
    "    else:\n",
    "        anomaly_flags = reconstruction_errors > used_threshold\n",
    "    \n",
    "    return anomaly_flags, used_threshold\n",
    "\n",
    "# Create sequences for simulation\n",
    "simulation_X, simulation_y = create_sequences(test_data_tail.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "# Create corresponding labels for evaluation\n",
    "simulation_labels = create_sequence_labels(test_labels_tail.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "\n",
    "forecast_list = []\n",
    "reconstruction_list = []\n",
    "reconstruction_errors = []\n",
    "true_windows = []\n",
    "\n",
    "# Process all samples without overlapping\n",
    "step_size = WINDOW_SIZE_SIMULATION  # Use the window size as step size to avoid overlap\n",
    "for i in range(0, len(simulation_X), step_size):\n",
    "    # Get the current window batch (up to step_size samples)\n",
    "    window_X = simulation_X[i:i+step_size]\n",
    "    window_y_true = simulation_y[i:i+step_size]\n",
    "    \n",
    "    if len(window_X) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Use LSTM Seq2Seq for forecasting\n",
    "    y_pred_future = best_model.predict(window_X, batch_size=128, verbose=1)\n",
    "    X_forecast = y_pred_future  # No need to expand dims as we're processing batches\n",
    "    \n",
    "    # Use Transformer Autoencoder for anomaly detection\n",
    "    y_reconstructed = transformer_ae.predict(X_forecast, batch_size=128, verbose=1)\n",
    "    \n",
    "    # Calculate reconstruction error for each sample in the batch\n",
    "    batch_reconstruction_errors = np.mean((y_pred_future - y_reconstructed)**2, axis=(1, 2))\n",
    "    \n",
    "    # Store predictions and errors\n",
    "    forecast_list.append(y_pred_future)\n",
    "    reconstruction_list.append(y_reconstructed)\n",
    "    reconstruction_errors.append(batch_reconstruction_errors)\n",
    "    true_windows.append(window_y_true)\n",
    "\n",
    "print(\"\\n✅ Real-time simulation complete.\")\n",
    "\n",
    "# Flatten the reconstruction errors list for threshold calculation\n",
    "all_reconstruction_errors = np.concatenate(reconstruction_errors)\n",
    "\n",
    "# Make sure we're only considering windows where we have both predictions and labels\n",
    "min_length = min(len(simulation_labels), len(all_reconstruction_errors))\n",
    "true_labels_subset = simulation_labels[:min_length]\n",
    "errors_subset = all_reconstruction_errors[:min_length]\n",
    "\n",
    "# Calculate optimal threshold using ROC curve analysis\n",
    "optimal_threshold = find_optimal_threshold_roc(true_labels_subset, errors_subset)\n",
    "\n",
    "# Calculate optimal percentile\n",
    "optimal_percentile = 100 * (1 - np.mean(np.where(all_reconstruction_errors >= optimal_threshold, 1, 0)))\n",
    "print(f\"✅ Optimal percentile corresponding to ROC threshold: {optimal_percentile:.5f}th percentile\")\n",
    "\n",
    "# Store optimal values to be used for manual control\n",
    "OPTIMAL_THRESHOLD = optimal_threshold\n",
    "OPTIMAL_PERCENTILE = optimal_percentile\n",
    "\n",
    "# Function to evaluate anomaly detection performance\n",
    "def evaluate_anomaly_detection(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"\\n📈 Anomaly Detection Evaluation:\")\n",
    "    print(f\"Precision: {precision:.5f}\")\n",
    "    print(f\"Recall:    {recall:.5f}\")\n",
    "    print(f\"F1 Score:  {f1:.5f}\")\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Example of using the manual threshold control\n",
    "# You can set your own threshold value or percentile\n",
    "MANUAL_THRESHOLD = None  # Set to a float value to use a specific threshold\n",
    "MANUAL_PERCENTILE = None  # Set to a percentile value (0-100) to use a percentile-based threshold\n",
    "\n",
    "# If both are None, it will use the optimal threshold from ROC curve\n",
    "anomaly_flags, used_threshold = detect_anomalies(\n",
    "    reconstruction_errors,\n",
    "    threshold=MANUAL_THRESHOLD,\n",
    "    percentile=MANUAL_PERCENTILE,\n",
    "    y_true=true_labels_subset\n",
    ")\n",
    "\n",
    "# Apply threshold to get anomaly flags (flattened)\n",
    "all_detected = np.concatenate([flags for flags in anomaly_flags])\n",
    "min_length = min(len(true_labels_subset), len(all_detected))\n",
    "all_detected_subset = all_detected[:min_length]\n",
    "\n",
    "# Evaluate anomaly detection performance using the current threshold\n",
    "precision, recall, f1 = evaluate_anomaly_detection(true_labels_subset[:min_length], all_detected_subset)\n",
    "\n",
    "# ========================\n",
    "# 10. VISUALIZATION WITH MANUAL THRESHOLD\n",
    "# ========================\n",
    "def plot_anomaly_detection(errors, labels, threshold, detected_anomalies=None):\n",
    "    \"\"\"\n",
    "    Plot reconstruction errors, threshold, and anomalies\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14,5))\n",
    "    \n",
    "    # Ensure we're only plotting up to the minimum length we have data for\n",
    "    min_plot_len = min(len(errors), len(labels))\n",
    "    plot_errors = errors[:min_plot_len]\n",
    "    plot_labels = labels[:min_plot_len]\n",
    "    \n",
    "    # Plot reconstruction errors\n",
    "    plt.plot(plot_errors, label='Reconstruction Error')\n",
    "    plt.axhline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.5f}')\n",
    "    \n",
    "    # Plot detected anomalies if provided\n",
    "    if detected_anomalies is not None:\n",
    "        detected_anomalies = detected_anomalies[:min_plot_len]\n",
    "        detected_indices = np.where(detected_anomalies == 1)[0]\n",
    "        plt.scatter(detected_indices,\n",
    "                    plot_errors[detected_indices],\n",
    "                    color='red', label='Detected Anomalies', s=10)\n",
    "    \n",
    "    # Plot true anomalies\n",
    "    true_anomaly_indices = np.where(plot_labels == 1)[0]\n",
    "    if len(true_anomaly_indices) > 0:\n",
    "        plt.scatter(true_anomaly_indices,\n",
    "                    np.ones_like(true_anomaly_indices) * np.max(plot_errors)*0.9,\n",
    "                    color='green', marker='*', label='True Anomalies', s=20)\n",
    "    else:\n",
    "        print(\"No true anomalies found in the subset of data being visualized\")\n",
    "    \n",
    "    plt.title(\"Reconstruction Errors vs True Anomalies\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Reconstruction Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Example of visualizing results with the current threshold\n",
    "plt_fig = plot_anomaly_detection(\n",
    "    all_reconstruction_errors, \n",
    "    true_labels_subset, \n",
    "    used_threshold, \n",
    "    all_detected\n",
    ")\n",
    "plt_fig.savefig(\"reconstruction_errors_with_threshold.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Analysis complete.\")\n",
    "print(f\"✅ Optimal threshold from ROC curve: {OPTIMAL_THRESHOLD:.5f}\")\n",
    "print(f\"✅ Optimal percentile: {OPTIMAL_PERCENTILE:.5f}th\")\n",
    "print(\"\\nTo use manual threshold control:\")\n",
    "print(\"1. Set MANUAL_THRESHOLD to a specific value, or\")\n",
    "print(\"2. Set MANUAL_PERCENTILE to use a percentile-based threshold\")\n",
    "print(\"3. If both are None, the optimal ROC threshold will be used\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
