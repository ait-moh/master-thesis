{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad884dbe-2345-4ccf-873a-314c475db55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU is available and will be used.\n",
      "✅ Found 'labels' column in detection dataset. Extracting for later evaluation.\n",
      "✅ Found 'labels' column in forecast dataset. Extracting for later evaluation.\n",
      "✅ Loaded test set with 242614 samples from ../../data/test_set.csv\n",
      "✅ Found 'labels' column in test set.\n",
      "✅ Scaled detection dataset shape: (373047, 26)\n",
      "✅ Scaled forecast dataset shape: (373047, 26)\n",
      "✅ Scaled test dataset shape: (242614, 26)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'split_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 147\u001b[0m\n\u001b[0;32m    145\u001b[0m split_idx_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(TEST_SIZE_POURCENTAGE) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_data_scaled)\n\u001b[0;32m    146\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data_scaled\u001b[38;5;241m.\u001b[39miloc[:split_idx_test]\n\u001b[1;32m--> 147\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m all_test_labels\u001b[38;5;241m.\u001b[39miloc[\u001b[43msplit_idx\u001b[49m:]\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Detection model - Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data_detection)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Testing samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Forecast model - Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Testing samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_idx' is not defined"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 0. IMPORTS\n",
    "# ========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, LSTM, RepeatVector, TimeDistributed,\n",
    "                                    MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D,\n",
    "                                    Conv1D)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "# ========================\n",
    "# 1. CONFIGURATION\n",
    "# ========================\n",
    "INPUT_STEPS = 10\n",
    "FORECAST_STEPS = 10\n",
    "TEST_RATIO = 0  # For validation split during training\n",
    "TEST_SIZE_POURCENTAGE = 1\n",
    "# Tuning parameters\n",
    "EPOCHS_LIST = [20]\n",
    "BATCH_SIZES = [128]\n",
    "WINDOW_SIZE_SIMULATION = 10  # 6h window\n",
    "# Transformer parameters for anomaly detection\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 256\n",
    "DROPOUT_RATE = 0.1\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ========================\n",
    "# 2. DEVICE SETUP\n",
    "# ========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(\"✅ GPU is available and will be used.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected, running on CPU.\")\n",
    "\n",
    "# ========================\n",
    "# 3. LOAD AND PREPROCESS DATA\n",
    "# ========================\n",
    "# Load detection dataset\n",
    "file_path_detection = '../../data/cleaned_labeled_dataset.csv'\n",
    "df_detection = pd.read_csv(file_path_detection, delimiter=',')\n",
    "df_detection['DateTime'] = pd.to_datetime(df_detection['DateTime'], errors='coerce')\n",
    "df_detection.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Load forecast dataset\n",
    "file_path_forecast = '../../data/preprocessed_data.csv'\n",
    "df_forecast = pd.read_csv(file_path_forecast, delimiter=',')\n",
    "df_forecast['DateTime'] = pd.to_datetime(df_forecast['DateTime'], errors='coerce')\n",
    "df_forecast.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Extract labels column if it exists from detection dataset\n",
    "if 'labels' in df_detection.columns:\n",
    "    print(\"✅ Found 'labels' column in detection dataset. Extracting for later evaluation.\")\n",
    "    labels_detection = df_detection['labels'].copy()\n",
    "    df_detection_train = df_detection.drop(columns=['labels'])\n",
    "else:\n",
    "    print(\"⚠️ No 'labels' column found in detection dataset. Will assume all samples are normal.\")\n",
    "    labels_detection = pd.Series(np.zeros(len(df_detection)))\n",
    "    df_detection_train = df_detection.copy()\n",
    "\n",
    "# Extract labels column if it exists from forecast dataset\n",
    "if 'labels' in df_forecast.columns:\n",
    "    print(\"✅ Found 'labels' column in forecast dataset. Extracting for later evaluation.\")\n",
    "    labels_forecast = df_forecast['labels'].copy()\n",
    "    df_forecast_train = df_forecast.drop(columns=['labels'])\n",
    "else:\n",
    "    print(\"⚠️ No 'labels' column found in forecast dataset. Will assume all samples are normal.\")\n",
    "    labels_forecast = pd.Series(np.zeros(len(df_forecast)))\n",
    "    df_forecast_train = df_forecast.copy()\n",
    "\n",
    "# ========================\n",
    "# 4. LOAD TEST SET\n",
    "# ========================\n",
    "# Load the test set from CSV\n",
    "test_set_path = '../../data/test_set.csv'\n",
    "df_test = pd.read_csv(test_set_path, delimiter=',')\n",
    "df_test['DateTime'] = pd.to_datetime(df_test['DateTime'], errors='coerce')\n",
    "df_test.set_index('DateTime', inplace=True)\n",
    "print(f\"✅ Loaded test set with {len(df_test)} samples from {test_set_path}\")\n",
    "\n",
    "# Extract labels from test set if they exist\n",
    "if 'labels' in df_test.columns:\n",
    "    print(\"✅ Found 'labels' column in test set.\")\n",
    "    all_test_labels = df_test['labels'].copy()\n",
    "    df_test_features = df_test.drop(columns=['labels'])\n",
    "else:\n",
    "    print(\"⚠️ No 'labels' column found in test set. Will assume all samples are normal.\")\n",
    "    all_test_labels = pd.Series(np.zeros(len(df_test)))\n",
    "    df_test_features = df_test.copy()\n",
    "\n",
    "# ========================\n",
    "# 5. NORMALIZE DATA\n",
    "# ========================\n",
    "# Normalize detection dataset\n",
    "scaler_detection = MinMaxScaler()\n",
    "scaled_data_detection = scaler_detection.fit_transform(df_detection_train.values)\n",
    "df_detection_scaled = pd.DataFrame(scaled_data_detection, index=df_detection_train.index, \n",
    "                                  columns=df_detection_train.columns).astype(np.float32)\n",
    "print(f\"✅ Scaled detection dataset shape: {df_detection_scaled.shape}\")\n",
    "\n",
    "# Normalize forecast dataset\n",
    "scaler_forecast = MinMaxScaler()\n",
    "scaled_data_forecast = scaler_forecast.fit_transform(df_forecast_train.values)\n",
    "df_forecast_scaled = pd.DataFrame(scaled_data_forecast, index=df_forecast_train.index, \n",
    "                                 columns=df_forecast_train.columns).astype(np.float32)\n",
    "print(f\"✅ Scaled forecast dataset shape: {df_forecast_scaled.shape}\")\n",
    "\n",
    "# Normalize test dataset - using the same scaler as detection dataset for consistency\n",
    "scaled_data_test = scaler_detection.transform(df_test_features.values)\n",
    "test_data_scaled = pd.DataFrame(scaled_data_test, index=df_test_features.index, \n",
    "                             columns=df_test_features.columns).astype(np.float32)\n",
    "print(f\"✅ Scaled test dataset shape: {test_data_scaled.shape}\")\n",
    "\n",
    "# ========================\n",
    "# 6. SEQUENTIAL TRAIN/TEST SPLIT FOR MODEL EVALUATION\n",
    "# ========================\n",
    "# Split for detection model\n",
    "split_idx_detection = int((1 - TEST_RATIO) * len(df_detection_scaled))\n",
    "train_data_detection = df_detection_scaled.iloc[:split_idx_detection]\n",
    "\n",
    "# Split for forecast model\n",
    "split_idx_forecast = int((1 - TEST_RATIO) * len(df_forecast_scaled))\n",
    "train_data = df_forecast_scaled.iloc[:split_idx_forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b165cb-92a9-4ae2-9db8-5f045866237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 5. CREATE SEQUENCES\n",
    "# ========================\n",
    "def create_sequences(data, input_steps, forecast_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_steps - forecast_steps):\n",
    "        X.append(data[i:i+input_steps])\n",
    "        y.append(data[i+input_steps:i+input_steps+forecast_steps])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Create sequence labels (a sequence is anomalous if any point in it is anomalous)\n",
    "def create_sequence_labels(labels, input_steps, forecast_steps):\n",
    "    \"\"\"\n",
    "    Create sequence-level labels from point-level labels.\n",
    "    A sequence is considered anomalous (1) if any point in its forecast window is anomalous.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array of point-level binary labels (0=normal, >0=anomaly)\n",
    "        input_steps: Number of input steps (not used for labeling)\n",
    "        forecast_steps: Number of forecast steps\n",
    "        \n",
    "    Returns:\n",
    "        Array of sequence-level binary labels\n",
    "    \"\"\"\n",
    "    seq_labels = []\n",
    "    for i in range(len(labels) - input_steps - forecast_steps):\n",
    "        # If any point in the forecast window is anomalous, mark the sequence as anomalous\n",
    "        forecast_window_labels = labels[i+input_steps:i+input_steps+forecast_steps]\n",
    "        # Convert any non-zero value to anomaly (1)\n",
    "        is_anomalous = np.any(np.array(forecast_window_labels) > 0)\n",
    "        seq_labels.append(1 if is_anomalous else 0)\n",
    "    return np.array(seq_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bdb1cc1-34c3-4df6-a193-888009480ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detection model - Training samples: 373047, Testing samples: 242614\n",
      "✅ Forecast model - Training samples: 373047, Testing samples: 242614\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE_POURCENTAGE = 0.7\n",
    "# Split for test set\n",
    "split_idx_test = int(TEST_SIZE_POURCENTAGE * len(test_data_scaled))\n",
    "test_data = test_data_scaled.iloc[:split_idx_test]\n",
    "\n",
    "test_labels = all_test_labels.iloc[:split_idx_test]\n",
    "print(f\"✅ Detection model - Training samples: {len(train_data_detection)}, Testing samples: {len(test_data)}\")\n",
    "print(f\"✅ Forecast model - Training samples: {len(train_data)}, Testing samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b05a1aea-2950-4d86-af33-d41a2cd0359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "2623/2623 [==============================] - 215s 77ms/step - loss: 0.0076 - val_loss: 0.0022\n",
      "Epoch 2/7\n",
      "2623/2623 [==============================] - 200s 76ms/step - loss: 7.1328e-04 - val_loss: 0.0016\n",
      "Epoch 3/7\n",
      "2623/2623 [==============================] - 194s 74ms/step - loss: 5.9179e-04 - val_loss: 0.0014\n",
      "Epoch 4/7\n",
      "2623/2623 [==============================] - 194s 74ms/step - loss: 5.4592e-04 - val_loss: 0.0014\n",
      "Epoch 5/7\n",
      "2623/2623 [==============================] - 188s 72ms/step - loss: 0.0022 - val_loss: 0.0107\n",
      "Epoch 6/7\n",
      "2623/2623 [==============================] - 191s 73ms/step - loss: 6.4040e-04 - val_loss: 0.0029\n",
      "Epoch 7/7\n",
      "2623/2623 [==============================] - 192s 73ms/step - loss: 0.0015 - val_loss: 0.0352\n",
      "\n",
      "✅ Transformer Autoencoder trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 8. BUILD AND TRAIN TRANSFORMER AUTOENCODER FOR ANOMALY DETECTION\n",
    "# ========================\n",
    "def create_ae_sequences(data, seq_len):\n",
    "    return np.array([data[i:i+seq_len] for i in range(len(data) - seq_len)], dtype=np.float32)\n",
    "\n",
    "X_ae_train = create_ae_sequences(train_data_detection.values, FORECAST_STEPS)\n",
    "\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    \n",
    "    # Add & Norm\n",
    "    return LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "\n",
    "def build_transformer_autoencoder(input_steps, input_dim, embed_dim=128, num_heads=4, ff_dim=256, dropout_rate=0.1):\n",
    "    inputs = Input(shape=(input_steps, input_dim))\n",
    "    \n",
    "    # Initial projection to embed_dim\n",
    "    x = Conv1D(filters=embed_dim, kernel_size=1, activation='relu')(inputs)\n",
    "    \n",
    "    # Encoder: Transformer blocks\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    # Bottleneck\n",
    "    encoded = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Decoder: Expand to sequence\n",
    "    x = RepeatVector(input_steps)(encoded)\n",
    "    \n",
    "    # Decoder: Transformer blocks\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    # Output projection back to original dimensions\n",
    "    outputs = TimeDistributed(Dense(input_dim))(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and train the transformer autoencoder for anomaly detection\n",
    "transformer_ae = build_transformer_autoencoder(\n",
    "    FORECAST_STEPS, \n",
    "    X_ae_train.shape[2],\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Train Transformer AE\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "transformer_ae.fit(\n",
    "    X_ae_train, \n",
    "    X_ae_train, \n",
    "    validation_split=0.1, \n",
    "    epochs=20, \n",
    "    batch_size=128, \n",
    "    callbacks=[es], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#transformer_ae.save(\"best_transformer_autoencoder.h5\")\n",
    "print(\"\\n✅ Transformer Autoencoder trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f06f9eea-835e-454e-98d1-04ea3030eacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83868"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a1e590-75df-4e78-91f4-d8917e6b4d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the last 3000 points of the test data for simulation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_data_tail \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m test_labels_tail \u001b[38;5;241m=\u001b[39m test_labels\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 9. REAL-TIME SIMULATION WITH MANUAL THRESHOLD CONTROL\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the last 3000 points of the test data for simulation\n",
    "test_data_tail = test_data.tail(10000).reset_index(drop=True)\n",
    "test_labels_tail = test_labels.tail(10000).reset_index(drop=True)\n",
    "\n",
    "# ========================\n",
    "# 9. REAL-TIME SIMULATION WITH MANUAL THRESHOLD CONTROL\n",
    "# ========================\n",
    "simulation_X, simulation_y = create_sequences(test_data_tail.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "# Create corresponding labels for evaluation\n",
    "simulation_labels = create_sequence_labels(test_labels_tail.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "\n",
    "#forecast_list = []\n",
    "#reconstruction_list = []\n",
    "reconstruction_errors = []\n",
    "#true_windows = []\n",
    "\n",
    "# Process all samples without overlapping\n",
    "step_size = WINDOW_SIZE_SIMULATION  # Use the window size as step size to avoid overlap\n",
    "for i in range(0, len(simulation_X), step_size):\n",
    "    # Get the current window batch (up to step_size samples)\n",
    "   # window_X = simulation_X[i:i+step_size]\n",
    "    window_y_true = simulation_y[i:i+step_size]\n",
    "    y_true = window_y_true\n",
    "    if len(window_X) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Use LSTM Seq2Seq for forecasting\n",
    "    #y_pred_future = best_model.predict(window_X, batch_size=128, verbose=1)\n",
    "    #X_forecast = y_pred_future  # No need to expand dims as we're processing batches\n",
    "    \n",
    "    # Use Transformer Autoencoder for anomaly detection\n",
    "    y_reconstructed = transformer_ae.predict(window_y_true, batch_size=128, verbose=1)\n",
    "    \n",
    "    # Calculate reconstruction error for each sample in the batch\n",
    "    batch_reconstruction_errors = np.mean((y_true - y_reconstructed)**2, axis=(1, 2))\n",
    "    \n",
    "    # Store predictions and errors\n",
    "    #forecast_list.append(y_pred_future)\n",
    "    #reconstruction_list.append(y_reconstructed)\n",
    "    reconstruction_errors.append(batch_reconstruction_errors)\n",
    "    #true_windows.append(window_y_true)\n",
    "\n",
    "print(\"\\n✅ Real-time simulation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223b3dd-0d11-4406-9bb2-848bd13fc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_THRESHOLD=None\n",
    "MANUAL_PERCENTILE=90\n",
    "# Flatten the reconstruction errors list for threshold calculation\n",
    "all_reconstruction_errors = np.concatenate(reconstruction_errors)\n",
    "\n",
    "# Make sure we're only considering windows where we have both predictions and labels\n",
    "min_length = min(len(simulation_labels), len(all_reconstruction_errors))\n",
    "true_labels_subset = simulation_labels[:min_length]\n",
    "errors_subset = all_reconstruction_errors[:min_length]\n",
    "\n",
    "# Determine threshold to use\n",
    "if MANUAL_THRESHOLD is not None:\n",
    "    # Use manually specified threshold\n",
    "    threshold = MANUAL_THRESHOLD\n",
    "    print(f\"\\n✅ Using manually specified threshold: {threshold:.5f}\")\n",
    "else:\n",
    "    # Use percentile-based threshold\n",
    "    threshold = np.percentile(all_reconstruction_errors, MANUAL_PERCENTILE)\n",
    "    print(f\"\\n✅ Using {MANUAL_PERCENTILE}th percentile threshold: {threshold:.5f}\")\n",
    "\n",
    "# Apply threshold to get anomaly flags\n",
    "anomaly_flags_list = [errors > threshold for errors in reconstruction_errors]\n",
    "all_detected = np.concatenate([flags for flags in anomaly_flags_list])\n",
    "\n",
    "# Make sure we're only considering windows where we have both predictions and labels\n",
    "min_length = min(len(true_labels_subset), len(all_detected))\n",
    "true_labels_subset = true_labels_subset[:min_length]\n",
    "all_detected_subset = all_detected[:min_length]\n",
    "\n",
    "# Print summary of anomalies\n",
    "print(f\"\\nAnomaly detection summary:\")\n",
    "print(f\"Number of true anomalies: {np.sum(true_labels_subset)}\")\n",
    "print(f\"Numberof detected anomalies: {np.sum(all_detected_subset)}\")\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "recall = recall_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "f1 = f1_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "\n",
    "print(f\"\\n📈 Anomaly Detection Evaluation:\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall:    {recall:.5f}\")\n",
    "print(f\"F1 Score:  {f1:.5f}\")\n",
    "\n",
    "# ========================\n",
    "# 10. PLOT RESULTS\n",
    "# ========================\n",
    "# Plot Reconstruction Errors with True Labels\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Ensure we're only plotting up to the minimum length we have data for\n",
    "min_plot_len = min(len(all_reconstruction_errors), len(true_labels_subset))\n",
    "plot_errors = all_reconstruction_errors[:min_plot_len]\n",
    "plot_labels = true_labels_subset[:min_plot_len]\n",
    "plot_detected = all_detected_subset[:min_plot_len]\n",
    "\n",
    "plt.plot(plot_errors, label='Reconstruction Error')\n",
    "plt.axhline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.5f}')\n",
    "\n",
    "# Detected anomalies\n",
    "detected_indices = np.where(plot_detected == 1)[0]\n",
    "plt.scatter(detected_indices,\n",
    "            plot_errors[detected_indices],\n",
    "            color='red', label='Detected Anomalies', s=10)\n",
    "\n",
    "# True anomalies\n",
    "true_anomaly_indices = np.where(plot_labels == 1)[0]\n",
    "if len(true_anomaly_indices) > 0:\n",
    "    plt.scatter(true_anomaly_indices,\n",
    "                np.ones_like(true_anomaly_indices) * np.max(plot_errors)*0.9,\n",
    "                color='green', marker='*', label='True Anomalies', s=20)\n",
    "else:\n",
    "    print(\"No true anomalies found in the subset of data being visualized\")\n",
    "\n",
    "plt.title(\"Reconstruction Errors vs True Anomalies\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reconstruction_errors_with_threshold.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Analysis complete.\")\n",
    "print(\"\\nTo change the threshold:\")\n",
    "print(\"1. Set MANUAL_THRESHOLD to a specific value to use that exact threshold\")\n",
    "print(\"2. Set MANUAL_PERCENTILE to use a percentile-based threshold (current: {}th)\".format(MANUAL_PERCENTILE))\n",
    "print(\"3. Re-run the script to see results with the new threshold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
