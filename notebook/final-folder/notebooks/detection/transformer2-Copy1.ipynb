{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad884dbe-2345-4ccf-873a-314c475db55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU is available and will be used.\n",
      "âœ… Found 'labels' column in detection dataset. Extracting for later evaluation.\n",
      "âœ… Found 'labels' column in forecast dataset. Extracting for later evaluation.\n",
      "âœ… Loaded test set with 242614 samples from ../../data/test_set.csv\n",
      "âœ… Found 'labels' column in test set.\n",
      "âœ… Scaled detection dataset shape: (373047, 26)\n",
      "âœ… Scaled forecast dataset shape: (373047, 26)\n",
      "âœ… Scaled test dataset shape: (242614, 26)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'split_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 147\u001b[0m\n\u001b[0;32m    145\u001b[0m split_idx_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(TEST_SIZE_POURCENTAGE) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_data_scaled)\n\u001b[0;32m    146\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data_scaled\u001b[38;5;241m.\u001b[39miloc[:split_idx_test]\n\u001b[1;32m--> 147\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m all_test_labels\u001b[38;5;241m.\u001b[39miloc[\u001b[43msplit_idx\u001b[49m:]\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Detection model - Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data_detection)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Testing samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Forecast model - Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Testing samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_idx' is not defined"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 0. IMPORTS\n",
    "# ========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, LSTM, RepeatVector, TimeDistributed,\n",
    "                                    MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D,\n",
    "                                    Conv1D)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "# ========================\n",
    "# 1. CONFIGURATION\n",
    "# ========================\n",
    "INPUT_STEPS = 10\n",
    "FORECAST_STEPS = 10\n",
    "TEST_RATIO = 0  # For validation split during training\n",
    "TEST_SIZE_POURCENTAGE = 1\n",
    "# Tuning parameters\n",
    "EPOCHS_LIST = [20]\n",
    "BATCH_SIZES = [128]\n",
    "WINDOW_SIZE_SIMULATION = 10  # 6h window\n",
    "# Transformer parameters for anomaly detection\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 256\n",
    "DROPOUT_RATE = 0.1\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ========================\n",
    "# 2. DEVICE SETUP\n",
    "# ========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(\"âœ… GPU is available and will be used.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected, running on CPU.\")\n",
    "\n",
    "# ========================\n",
    "# 3. LOAD AND PREPROCESS DATA\n",
    "# ========================\n",
    "# Load detection dataset\n",
    "file_path_detection = '../../data/cleaned_labeled_dataset.csv'\n",
    "df_detection = pd.read_csv(file_path_detection, delimiter=',')\n",
    "df_detection['DateTime'] = pd.to_datetime(df_detection['DateTime'], errors='coerce')\n",
    "df_detection.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Load forecast dataset\n",
    "file_path_forecast = '../../data/preprocessed_data.csv'\n",
    "df_forecast = pd.read_csv(file_path_forecast, delimiter=',')\n",
    "df_forecast['DateTime'] = pd.to_datetime(df_forecast['DateTime'], errors='coerce')\n",
    "df_forecast.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Extract labels column if it exists from detection dataset\n",
    "if 'labels' in df_detection.columns:\n",
    "    print(\"âœ… Found 'labels' column in detection dataset. Extracting for later evaluation.\")\n",
    "    labels_detection = df_detection['labels'].copy()\n",
    "    df_detection_train = df_detection.drop(columns=['labels'])\n",
    "else:\n",
    "    print(\"âš ï¸ No 'labels' column found in detection dataset. Will assume all samples are normal.\")\n",
    "    labels_detection = pd.Series(np.zeros(len(df_detection)))\n",
    "    df_detection_train = df_detection.copy()\n",
    "\n",
    "# Extract labels column if it exists from forecast dataset\n",
    "if 'labels' in df_forecast.columns:\n",
    "    print(\"âœ… Found 'labels' column in forecast dataset. Extracting for later evaluation.\")\n",
    "    labels_forecast = df_forecast['labels'].copy()\n",
    "    df_forecast_train = df_forecast.drop(columns=['labels'])\n",
    "else:\n",
    "    print(\"âš ï¸ No 'labels' column found in forecast dataset. Will assume all samples are normal.\")\n",
    "    labels_forecast = pd.Series(np.zeros(len(df_forecast)))\n",
    "    df_forecast_train = df_forecast.copy()\n",
    "\n",
    "# ========================\n",
    "# 4. LOAD TEST SET\n",
    "# ========================\n",
    "# Load the test set from CSV\n",
    "test_set_path = '../../data/test_set.csv'\n",
    "df_test = pd.read_csv(test_set_path, delimiter=',')\n",
    "df_test['DateTime'] = pd.to_datetime(df_test['DateTime'], errors='coerce')\n",
    "df_test.set_index('DateTime', inplace=True)\n",
    "print(f\"âœ… Loaded test set with {len(df_test)} samples from {test_set_path}\")\n",
    "\n",
    "# Extract labels from test set if they exist\n",
    "if 'labels' in df_test.columns:\n",
    "    print(\"âœ… Found 'labels' column in test set.\")\n",
    "    all_test_labels = df_test['labels'].copy()\n",
    "    df_test_features = df_test.drop(columns=['labels'])\n",
    "else:\n",
    "    print(\"âš ï¸ No 'labels' column found in test set. Will assume all samples are normal.\")\n",
    "    all_test_labels = pd.Series(np.zeros(len(df_test)))\n",
    "    df_test_features = df_test.copy()\n",
    "\n",
    "# ========================\n",
    "# 5. NORMALIZE DATA\n",
    "# ========================\n",
    "# Normalize detection dataset\n",
    "scaler_detection = MinMaxScaler()\n",
    "scaled_data_detection = scaler_detection.fit_transform(df_detection_train.values)\n",
    "df_detection_scaled = pd.DataFrame(scaled_data_detection, index=df_detection_train.index, \n",
    "                                  columns=df_detection_train.columns).astype(np.float32)\n",
    "print(f\"âœ… Scaled detection dataset shape: {df_detection_scaled.shape}\")\n",
    "\n",
    "# Normalize forecast dataset\n",
    "scaler_forecast = MinMaxScaler()\n",
    "scaled_data_forecast = scaler_forecast.fit_transform(df_forecast_train.values)\n",
    "df_forecast_scaled = pd.DataFrame(scaled_data_forecast, index=df_forecast_train.index, \n",
    "                                 columns=df_forecast_train.columns).astype(np.float32)\n",
    "print(f\"âœ… Scaled forecast dataset shape: {df_forecast_scaled.shape}\")\n",
    "\n",
    "# Normalize test dataset - using the same scaler as detection dataset for consistency\n",
    "scaled_data_test = scaler_detection.transform(df_test_features.values)\n",
    "test_data_scaled = pd.DataFrame(scaled_data_test, index=df_test_features.index, \n",
    "                             columns=df_test_features.columns).astype(np.float32)\n",
    "print(f\"âœ… Scaled test dataset shape: {test_data_scaled.shape}\")\n",
    "\n",
    "# ========================\n",
    "# 6. SEQUENTIAL TRAIN/TEST SPLIT FOR MODEL EVALUATION\n",
    "# ========================\n",
    "# Split for detection model\n",
    "split_idx_detection = int((1 - TEST_RATIO) * len(df_detection_scaled))\n",
    "train_data_detection = df_detection_scaled.iloc[:split_idx_detection]\n",
    "\n",
    "# Split for forecast model\n",
    "split_idx_forecast = int((1 - TEST_RATIO) * len(df_forecast_scaled))\n",
    "train_data = df_forecast_scaled.iloc[:split_idx_forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b165cb-92a9-4ae2-9db8-5f045866237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 5. CREATE SEQUENCES\n",
    "# ========================\n",
    "def create_sequences(data, input_steps, forecast_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_steps - forecast_steps):\n",
    "        X.append(data[i:i+input_steps])\n",
    "        y.append(data[i+input_steps:i+input_steps+forecast_steps])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Create sequence labels (a sequence is anomalous if any point in it is anomalous)\n",
    "def create_sequence_labels(labels, input_steps, forecast_steps):\n",
    "    \"\"\"\n",
    "    Create sequence-level labels from point-level labels.\n",
    "    A sequence is considered anomalous (1) if any point in its forecast window is anomalous.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array of point-level binary labels (0=normal, >0=anomaly)\n",
    "        input_steps: Number of input steps (not used for labeling)\n",
    "        forecast_steps: Number of forecast steps\n",
    "        \n",
    "    Returns:\n",
    "        Array of sequence-level binary labels\n",
    "    \"\"\"\n",
    "    seq_labels = []\n",
    "    for i in range(len(labels) - input_steps - forecast_steps):\n",
    "        # If any point in the forecast window is anomalous, mark the sequence as anomalous\n",
    "        forecast_window_labels = labels[i+input_steps:i+input_steps+forecast_steps]\n",
    "        # Convert any non-zero value to anomaly (1)\n",
    "        is_anomalous = np.any(np.array(forecast_window_labels) > 0)\n",
    "        seq_labels.append(1 if is_anomalous else 0)\n",
    "    return np.array(seq_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bdb1cc1-34c3-4df6-a193-888009480ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Detection model - Training samples: 373047, Testing samples: 242614\n",
      "âœ… Forecast model - Training samples: 373047, Testing samples: 242614\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE_POURCENTAGE = 0.7\n",
    "# Split for test set\n",
    "split_idx_test = int(TEST_SIZE_POURCENTAGE * len(test_data_scaled))\n",
    "test_data = test_data_scaled.iloc[:split_idx_test]\n",
    "\n",
    "test_labels = all_test_labels.iloc[:split_idx_test]\n",
    "print(f\"âœ… Detection model - Training samples: {len(train_data_detection)}, Testing samples: {len(test_data)}\")\n",
    "print(f\"âœ… Forecast model - Training samples: {len(train_data)}, Testing samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b05a1aea-2950-4d86-af33-d41a2cd0359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "2623/2623 [==============================] - 215s 77ms/step - loss: 0.0076 - val_loss: 0.0022\n",
      "Epoch 2/7\n",
      "2623/2623 [==============================] - 200s 76ms/step - loss: 7.1328e-04 - val_loss: 0.0016\n",
      "Epoch 3/7\n",
      "2623/2623 [==============================] - 194s 74ms/step - loss: 5.9179e-04 - val_loss: 0.0014\n",
      "Epoch 4/7\n",
      "2623/2623 [==============================] - 194s 74ms/step - loss: 5.4592e-04 - val_loss: 0.0014\n",
      "Epoch 5/7\n",
      "2623/2623 [==============================] - 188s 72ms/step - loss: 0.0022 - val_loss: 0.0107\n",
      "Epoch 6/7\n",
      "2623/2623 [==============================] - 191s 73ms/step - loss: 6.4040e-04 - val_loss: 0.0029\n",
      "Epoch 7/7\n",
      "2623/2623 [==============================] - 192s 73ms/step - loss: 0.0015 - val_loss: 0.0352\n",
      "\n",
      "âœ… Transformer Autoencoder trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 8. BUILD AND TRAIN TRANSFORMER AUTOENCODER FOR ANOMALY DETECTION\n",
    "# ========================\n",
    "def create_ae_sequences(data, seq_len):\n",
    "    return np.array([data[i:i+seq_len] for i in range(len(data) - seq_len)], dtype=np.float32)\n",
    "\n",
    "X_ae_train = create_ae_sequences(train_data_detection.values, FORECAST_STEPS)\n",
    "\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    \n",
    "    # Add & Norm\n",
    "    return LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "\n",
    "def build_transformer_autoencoder(input_steps, input_dim, embed_dim=128, num_heads=4, ff_dim=256, dropout_rate=0.1):\n",
    "    inputs = Input(shape=(input_steps, input_dim))\n",
    "    \n",
    "    # Initial projection to embed_dim\n",
    "    x = Conv1D(filters=embed_dim, kernel_size=1, activation='relu')(inputs)\n",
    "    \n",
    "    # Encoder: Transformer blocks\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    # Bottleneck\n",
    "    encoded = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Decoder: Expand to sequence\n",
    "    x = RepeatVector(input_steps)(encoded)\n",
    "    \n",
    "    # Decoder: Transformer blocks\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    # Output projection back to original dimensions\n",
    "    outputs = TimeDistributed(Dense(input_dim))(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and train the transformer autoencoder for anomaly detection\n",
    "transformer_ae = build_transformer_autoencoder(\n",
    "    FORECAST_STEPS, \n",
    "    X_ae_train.shape[2],\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Train Transformer AE\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "transformer_ae.fit(\n",
    "    X_ae_train, \n",
    "    X_ae_train, \n",
    "    validation_split=0.1, \n",
    "    epochs=20, \n",
    "    batch_size=128, \n",
    "    callbacks=[es], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#transformer_ae.save(\"best_transformer_autoencoder.h5\")\n",
    "print(\"\\nâœ… Transformer Autoencoder trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f06f9eea-835e-454e-98d1-04ea3030eacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83868"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a1e590-75df-4e78-91f4-d8917e6b4d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the last 3000 points of the test data for simulation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_data_tail \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m test_labels_tail \u001b[38;5;241m=\u001b[39m test_labels\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 9. REAL-TIME SIMULATION WITH MANUAL THRESHOLD CONTROL\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the last 3000 points of the test data for simulation\n",
    "test_data_tail = test_data.tail(10000).reset_index(drop=True)\n",
    "test_labels_tail = test_labels.tail(10000).reset_index(drop=True)\n",
    "\n",
    "# ========================\n",
    "# 9. REAL-TIME SIMULATION WITH MANUAL THRESHOLD CONTROL\n",
    "# ========================\n",
    "simulation_X, simulation_y = create_sequences(test_data_tail.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "# Create corresponding labels for evaluation\n",
    "simulation_labels = create_sequence_labels(test_labels_tail.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "\n",
    "#forecast_list = []\n",
    "#reconstruction_list = []\n",
    "reconstruction_errors = []\n",
    "#true_windows = []\n",
    "\n",
    "# Process all samples without overlapping\n",
    "step_size = WINDOW_SIZE_SIMULATION  # Use the window size as step size to avoid overlap\n",
    "for i in range(0, len(simulation_X), step_size):\n",
    "    # Get the current window batch (up to step_size samples)\n",
    "   # window_X = simulation_X[i:i+step_size]\n",
    "    window_y_true = simulation_y[i:i+step_size]\n",
    "    y_true = window_y_true\n",
    "    if len(window_X) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Use LSTM Seq2Seq for forecasting\n",
    "    #y_pred_future = best_model.predict(window_X, batch_size=128, verbose=1)\n",
    "    #X_forecast = y_pred_future  # No need to expand dims as we're processing batches\n",
    "    \n",
    "    # Use Transformer Autoencoder for anomaly detection\n",
    "    y_reconstructed = transformer_ae.predict(window_y_true, batch_size=128, verbose=1)\n",
    "    \n",
    "    # Calculate reconstruction error for each sample in the batch\n",
    "    batch_reconstruction_errors = np.mean((y_true - y_reconstructed)**2, axis=(1, 2))\n",
    "    \n",
    "    # Store predictions and errors\n",
    "    #forecast_list.append(y_pred_future)\n",
    "    #reconstruction_list.append(y_reconstructed)\n",
    "    reconstruction_errors.append(batch_reconstruction_errors)\n",
    "    #true_windows.append(window_y_true)\n",
    "\n",
    "print(\"\\nâœ… Real-time simulation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223b3dd-0d11-4406-9bb2-848bd13fc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_THRESHOLD=None\n",
    "MANUAL_PERCENTILE=90\n",
    "# Flatten the reconstruction errors list for threshold calculation\n",
    "all_reconstruction_errors = np.concatenate(reconstruction_errors)\n",
    "\n",
    "# Make sure we're only considering windows where we have both predictions and labels\n",
    "min_length = min(len(simulation_labels), len(all_reconstruction_errors))\n",
    "true_labels_subset = simulation_labels[:min_length]\n",
    "errors_subset = all_reconstruction_errors[:min_length]\n",
    "\n",
    "# Determine threshold to use\n",
    "if MANUAL_THRESHOLD is not None:\n",
    "    # Use manually specified threshold\n",
    "    threshold = MANUAL_THRESHOLD\n",
    "    print(f\"\\nâœ… Using manually specified threshold: {threshold:.5f}\")\n",
    "else:\n",
    "    # Use percentile-based threshold\n",
    "    threshold = np.percentile(all_reconstruction_errors, MANUAL_PERCENTILE)\n",
    "    print(f\"\\nâœ… Using {MANUAL_PERCENTILE}th percentile threshold: {threshold:.5f}\")\n",
    "\n",
    "# Apply threshold to get anomaly flags\n",
    "anomaly_flags_list = [errors > threshold for errors in reconstruction_errors]\n",
    "all_detected = np.concatenate([flags for flags in anomaly_flags_list])\n",
    "\n",
    "# Make sure we're only considering windows where we have both predictions and labels\n",
    "min_length = min(len(true_labels_subset), len(all_detected))\n",
    "true_labels_subset = true_labels_subset[:min_length]\n",
    "all_detected_subset = all_detected[:min_length]\n",
    "\n",
    "# Print summary of anomalies\n",
    "print(f\"\\nAnomaly detection summary:\")\n",
    "print(f\"Number of true anomalies: {np.sum(true_labels_subset)}\")\n",
    "print(f\"Numberof detected anomalies: {np.sum(all_detected_subset)}\")\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "recall = recall_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "f1 = f1_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Anomaly Detection Evaluation:\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall:    {recall:.5f}\")\n",
    "print(f\"F1 Score:  {f1:.5f}\")\n",
    "\n",
    "# ========================\n",
    "# 10. PLOT RESULTS\n",
    "# ========================\n",
    "# Plot Reconstruction Errors with True Labels\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Ensure we're only plotting up to the minimum length we have data for\n",
    "min_plot_len = min(len(all_reconstruction_errors), len(true_labels_subset))\n",
    "plot_errors = all_reconstruction_errors[:min_plot_len]\n",
    "plot_labels = true_labels_subset[:min_plot_len]\n",
    "plot_detected = all_detected_subset[:min_plot_len]\n",
    "\n",
    "plt.plot(plot_errors, label='Reconstruction Error')\n",
    "plt.axhline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.5f}')\n",
    "\n",
    "# Detected anomalies\n",
    "detected_indices = np.where(plot_detected == 1)[0]\n",
    "plt.scatter(detected_indices,\n",
    "            plot_errors[detected_indices],\n",
    "            color='red', label='Detected Anomalies', s=10)\n",
    "\n",
    "# True anomalies\n",
    "true_anomaly_indices = np.where(plot_labels == 1)[0]\n",
    "if len(true_anomaly_indices) > 0:\n",
    "    plt.scatter(true_anomaly_indices,\n",
    "                np.ones_like(true_anomaly_indices) * np.max(plot_errors)*0.9,\n",
    "                color='green', marker='*', label='True Anomalies', s=20)\n",
    "else:\n",
    "    print(\"No true anomalies found in the subset of data being visualized\")\n",
    "\n",
    "plt.title(\"Reconstruction Errors vs True Anomalies\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reconstruction_errors_with_threshold.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Analysis complete.\")\n",
    "print(\"\\nTo change the threshold:\")\n",
    "print(\"1. Set MANUAL_THRESHOLD to a specific value to use that exact threshold\")\n",
    "print(\"2. Set MANUAL_PERCENTILE to use a percentile-based threshold (current: {}th)\".format(MANUAL_PERCENTILE))\n",
    "print(\"3. Re-run the script to see results with the new threshold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
