{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c64a02-0c63-492e-8fd3-deb1d3957d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU is available and will be used.\n",
      "âœ… Original dataset shape: (12131, 26)\n",
      "âœ… Cleaned dataset shape: (18652, 26)\n",
      "âœ… Training sequences shape: (18642, 10, 26)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 26)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 10, 128)      3456        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 10, 128)     263808      ['conv1d[0][0]',                 \n",
      " dAttention)                                                      'conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 10, 128)      0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 10, 128)     0           ['conv1d[0][0]',                 \n",
      " da)                                                              'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 10, 128)     256         ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10, 256)      33024       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10, 128)      32896       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 10, 128)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 10, 128)     0           ['layer_normalization[0][0]',    \n",
      " mbda)                                                            'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 10, 128)     256         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 10, 128)     263808      ['layer_normalization_1[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10, 128)      0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 10, 128)     0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 10, 128)     256         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10, 256)      33024       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10, 128)      32896       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10, 128)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 10, 128)     0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 10, 128)     256         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['layer_normalization_3[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 10, 128)      0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 10, 128)     263808      ['repeat_vector[0][0]',          \n",
      " eadAttention)                                                    'repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 10, 128)      0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 10, 128)     0           ['repeat_vector[0][0]',          \n",
      " mbda)                                                            'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 10, 128)     256         ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 10, 256)      33024       ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 10, 128)      32896       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 10, 128)      0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 10, 128)     0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 10, 128)     256         ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 10, 128)     263808      ['layer_normalization_5[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 10, 128)      0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 10, 128)     0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 10, 128)     256         ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 10, 256)      33024       ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 10, 128)      32896       ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 10, 128)      0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 10, 128)     0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 10, 128)     256         ['tf.__operators__.add_7[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 10, 26)      3354        ['layer_normalization_7[0][0]']  \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,327,770\n",
      "Trainable params: 1,327,770\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "263/263 [==============================] - 36s 84ms/step - loss: 0.0611 - val_loss: 0.0190\n",
      "Epoch 2/20\n",
      "263/263 [==============================] - 19s 74ms/step - loss: 0.0068 - val_loss: 0.0086\n",
      "Epoch 3/20\n",
      "263/263 [==============================] - 20s 75ms/step - loss: 0.0042 - val_loss: 0.0087\n",
      "Epoch 4/20\n",
      "263/263 [==============================] - 19s 74ms/step - loss: 0.0034 - val_loss: 0.0057\n",
      "Epoch 5/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0029 - val_loss: 0.0076\n",
      "Epoch 6/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0027 - val_loss: 0.0059\n",
      "Epoch 7/20\n",
      "263/263 [==============================] - 20s 76ms/step - loss: 0.0025 - val_loss: 0.0055\n",
      "Epoch 8/20\n",
      "263/263 [==============================] - 19s 72ms/step - loss: 0.0024 - val_loss: 0.0052\n",
      "Epoch 9/20\n",
      "263/263 [==============================] - 20s 74ms/step - loss: 0.0023 - val_loss: 0.0052\n",
      "Epoch 10/20\n",
      "263/263 [==============================] - 19s 74ms/step - loss: 0.0022 - val_loss: 0.0053\n",
      "Epoch 11/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0022 - val_loss: 0.0044\n",
      "Epoch 12/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0021 - val_loss: 0.0041\n",
      "Epoch 13/20\n",
      "263/263 [==============================] - 20s 75ms/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 14/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0021 - val_loss: 0.0039\n",
      "Epoch 15/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0020 - val_loss: 0.0037\n",
      "Epoch 16/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0020 - val_loss: 0.0037\n",
      "Epoch 17/20\n",
      "263/263 [==============================] - 20s 75ms/step - loss: 0.0020 - val_loss: 0.0038\n",
      "Epoch 18/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 19/20\n",
      "263/263 [==============================] - 19s 73ms/step - loss: 0.0020 - val_loss: 0.0042\n",
      "Epoch 20/20\n",
      "263/263 [==============================] - 20s 74ms/step - loss: 0.0020 - val_loss: 0.0037\n",
      "\n",
      "âœ… Transformer Autoencoder trained and saved.\n",
      "âœ… Test data shape: (3000, 26)\n",
      "âœ… Test sequences shape: (2990, 10, 26)\n",
      "\n",
      "Anomaly Detection Metrics:\n",
      "Precision: 0.0833\n",
      "Recall: 0.0336\n",
      "F1 Score: 0.0479\n",
      "\n",
      "Confusion Matrix Details:\n",
      "True Positives: 25\n",
      "False Positives: 275\n",
      "True Negatives: 1981\n",
      "False Negatives: 719\n",
      "Total Anomaly Timesteps in Ground Truth: 744\n",
      "Total Timesteps Flagged as Anomalies: 300\n",
      "\n",
      "âœ… Metrics saved to 'transformer_ae_anomaly_detection_metrics.csv'.\n",
      "\n",
      "âœ… All visualizations saved.\n",
      "\n",
      "Threshold Tuning:\n",
      "Threshold percentile: 20.0%\n",
      "Precision: 0.1400, Recall: 0.1129, F1: 0.1250\n",
      "Threshold percentile: 15.0%\n",
      "Precision: 0.1222, Recall: 0.0739, F1: 0.0921\n",
      "Threshold percentile: 10.0%\n",
      "Precision: 0.0833, Recall: 0.0336, F1: 0.0479\n",
      "Threshold percentile: 5.0%\n",
      "Precision: 0.0267, Recall: 0.0054, F1: 0.0089\n",
      "Threshold percentile: 3.0%\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Threshold percentile: 1.0%\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "Best Threshold Found: 0.004767\n",
      "Best Precision: 0.1400\n",
      "Best Recall: 0.1129\n",
      "Best F1 Score: 0.1250\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 0. IMPORTS\n",
    "# ========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, RepeatVector, TimeDistributed,\n",
    "                                    MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D,\n",
    "                                    Conv1D)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "\n",
    "# ========================\n",
    "# 1. CONFIGURATION\n",
    "# ========================\n",
    "FORECAST_STEPS = 10  # Window size for autoencoder\n",
    "THRESHOLD_PERCENTILE = 90\n",
    "# Transformer parameters for anomaly detection\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 256\n",
    "DROPOUT_RATE = 0.1\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ========================\n",
    "# 2. DEVICE SETUP\n",
    "# ========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(\"âœ… GPU is available and will be used.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected, running on CPU.\")\n",
    "\n",
    "# ========================\n",
    "# 3. LOAD AND PREPROCESS DATA\n",
    "# ========================\n",
    "# Load original labeled dataset and parse DateTime column\n",
    "original_df = pd.read_csv('../../data/test_set.csv')\n",
    "original_df['DateTime'] = pd.to_datetime(original_df['DateTime'], errors='coerce')\n",
    "original_df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Load cleaned labeled dataset and parse DateTime column\n",
    "cleaned_df = pd.read_csv('../../data/cleaned_labeled_dataset.csv')\n",
    "cleaned_df['DateTime'] = pd.to_datetime(cleaned_df['DateTime'], errors='coerce')\n",
    "cleaned_df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Separate out labels for evaluation\n",
    "original_labels = original_df['labels'].copy()\n",
    "original_df.drop(columns=['labels'], inplace=True)\n",
    "cleaned_df.drop(columns=['labels'], inplace=True)\n",
    "\n",
    "# Drop columns with more than 30% missing values and fill the rest\n",
    "original_df.dropna(axis=1, thresh=int(0.7 * len(original_df)), inplace=True)\n",
    "original_df.ffill(inplace=True)  # Forward fill\n",
    "original_df.bfill(inplace=True)  # Backward fill\n",
    "\n",
    "cleaned_df.dropna(axis=1, thresh=int(0.7 * len(cleaned_df)), inplace=True)\n",
    "cleaned_df.ffill(inplace=True)\n",
    "cleaned_df.bfill(inplace=True)\n",
    "\n",
    "# Normalize data using MinMaxScaler to scale features to [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "original_scaled = scaler.fit_transform(original_df.values)  # Fit on original data\n",
    "cleaned_scaled = scaler.transform(cleaned_df.values)  # Transform cleaned data using the same scaler\n",
    "\n",
    "# Convert normalized arrays back to DataFrames, preserving index and column names\n",
    "original_df_scaled = pd.DataFrame(original_scaled, index=original_df.index, columns=original_df.columns).astype(np.float32)\n",
    "cleaned_df_scaled = pd.DataFrame(cleaned_scaled, index=cleaned_df.index, columns=cleaned_df.columns).astype(np.float32)\n",
    "\n",
    "print(f\"âœ… Original dataset shape: {original_df_scaled.shape}\")\n",
    "print(f\"âœ… Cleaned dataset shape: {cleaned_df_scaled.shape}\")\n",
    "\n",
    "# ========================\n",
    "# 4. CREATE SEQUENCES FOR AUTOENCODER\n",
    "# ========================\n",
    "def create_ae_sequences(data, seq_len):\n",
    "    \"\"\"Create fixed-length sequences for autoencoder (input == output)\"\"\"\n",
    "    return np.array([data[i:i+seq_len] for i in range(len(data) - seq_len)], dtype=np.float32)\n",
    "\n",
    "# Create sequences for training the autoencoder from cleaned data\n",
    "X_ae_train = create_ae_sequences(cleaned_df_scaled.values, FORECAST_STEPS)\n",
    "print(f\"âœ… Training sequences shape: {X_ae_train.shape}\")\n",
    "\n",
    "# ========================\n",
    "# 5. BUILD TRANSFORMER AUTOENCODER\n",
    "# ========================\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"Transformer encoder block\"\"\"\n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    \n",
    "    # Add & Norm\n",
    "    return LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "\n",
    "def build_transformer_autoencoder(input_steps, input_dim, embed_dim=128, num_heads=4, ff_dim=256, dropout_rate=0.1):\n",
    "    \"\"\"Build a transformer-based autoencoder for anomaly detection\"\"\"\n",
    "    inputs = Input(shape=(input_steps, input_dim))\n",
    "    \n",
    "    # Initial projection to embed_dim\n",
    "    x = Conv1D(filters=embed_dim, kernel_size=1, activation='relu')(inputs)\n",
    "    \n",
    "    # Encoder: Transformer blocks\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    # Bottleneck\n",
    "    encoded = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Decoder: Expand to sequence\n",
    "    x = RepeatVector(input_steps)(encoded)\n",
    "    \n",
    "    # Decoder: Transformer blocks\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    # Output projection back to original dimensions\n",
    "    outputs = TimeDistributed(Dense(input_dim))(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# ========================\n",
    "# 6. TRAIN TRANSFORMER AUTOENCODER\n",
    "# ========================\n",
    "# Build the transformer autoencoder\n",
    "transformer_ae = build_transformer_autoencoder(\n",
    "    FORECAST_STEPS, \n",
    "    X_ae_train.shape[2],\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "transformer_ae.summary()\n",
    "\n",
    "# Train with early stopping\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history = transformer_ae.fit(\n",
    "    X_ae_train, \n",
    "    X_ae_train, \n",
    "    validation_split=0.1, \n",
    "    epochs=20, \n",
    "    batch_size=64, \n",
    "    callbacks=[es], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model\n",
    "\n",
    "print(\"\\nâœ… Transformer Autoencoder trained and saved.\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Transformer Autoencoder Training History')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"transformer_ae_training_history.png\")\n",
    "plt.close()\n",
    "\n",
    "# ========================\n",
    "# 7. EVALUATION ON TEST DATA\n",
    "# ========================\n",
    "# Use the last 3000 entries from the original dataset for testing\n",
    "test_data = original_df_scaled.tail(3000).reset_index(drop=True)\n",
    "test_labels = original_labels.tail(3000).reset_index(drop=True)\n",
    "print(f\"âœ… Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Create sequences for testing\n",
    "test_sequences = create_ae_sequences(test_data.values, FORECAST_STEPS)\n",
    "print(f\"âœ… Test sequences shape: {test_sequences.shape}\")\n",
    "\n",
    "# Process all test sequences in batches for memory efficiency\n",
    "reconstruction_errors = []\n",
    "batch_size = 128\n",
    "for i in range(0, len(test_sequences), batch_size):\n",
    "    batch = test_sequences[i:min(i+batch_size, len(test_sequences))]\n",
    "    # Get reconstructions\n",
    "    reconstructions = transformer_ae.predict(batch, verbose=0)\n",
    "    # Calculate reconstruction error for each sequence\n",
    "    for j in range(len(batch)):\n",
    "        original = batch[j]\n",
    "        reconstructed = reconstructions[j]\n",
    "        # Calculate MSE for each timestep across all features\n",
    "        error_per_timestep = np.mean((original - reconstructed)**2, axis=1)\n",
    "        reconstruction_errors.append(error_per_timestep)\n",
    "\n",
    "# Convert to numpy array for easier manipulation\n",
    "reconstruction_errors = np.array(reconstruction_errors)\n",
    "\n",
    "# Calculate error scores: aggregate multiple predictions for each timestep\n",
    "timesteps_total = len(test_data) - FORECAST_STEPS + 1  # Number of sequences\n",
    "aggregated_errors = np.zeros(timesteps_total + FORECAST_STEPS - 1)  # Total timesteps covered\n",
    "counts = np.zeros(timesteps_total + FORECAST_STEPS - 1)  # Count how many predictions for each timestep\n",
    "\n",
    "# For each sequence\n",
    "for i in range(len(reconstruction_errors)):\n",
    "    # For each timestep in the sequence\n",
    "    for j in range(FORECAST_STEPS):\n",
    "        # Add error to the corresponding position in the original time series\n",
    "        aggregated_errors[i + j] += reconstruction_errors[i][j]\n",
    "        counts[i + j] += 1\n",
    "\n",
    "# Average the errors where we have multiple predictions\n",
    "aggregated_errors = np.divide(aggregated_errors, counts, where=counts>0)\n",
    "\n",
    "# Calculate threshold\n",
    "threshold = np.percentile(aggregated_errors[counts > 0], THRESHOLD_PERCENTILE)\n",
    "\n",
    "# Generate anomaly flags\n",
    "anomaly_flags = (aggregated_errors > threshold).astype(int)\n",
    "\n",
    "# Align with original labels\n",
    "# The labels need to be aligned with the outputs of our autoencoder\n",
    "aligned_labels = test_labels.values[:len(anomaly_flags)]\n",
    "\n",
    "# Make sure both arrays are the same length\n",
    "min_len = min(len(anomaly_flags), len(aligned_labels))\n",
    "anomaly_flags = anomaly_flags[:min_len]\n",
    "aligned_labels = aligned_labels[:min_len]\n",
    "\n",
    "# ========================\n",
    "# 8. COMPUTE METRICS\n",
    "# ========================\n",
    "precision = precision_score(aligned_labels, anomaly_flags, zero_division=0)\n",
    "recall = recall_score(aligned_labels, anomaly_flags, zero_division=0)\n",
    "f1 = f1_score(aligned_labels, anomaly_flags, zero_division=0)\n",
    "\n",
    "print(\"\\nAnomaly Detection Metrics:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate confusion matrix values for detailed analysis\n",
    "true_positives = ((aligned_labels == 1) & (anomaly_flags == 1)).sum()\n",
    "false_positives = ((aligned_labels == 0) & (anomaly_flags == 1)).sum()\n",
    "true_negatives = ((aligned_labels == 0) & (anomaly_flags == 0)).sum()\n",
    "false_negatives = ((aligned_labels == 1) & (anomaly_flags == 0)).sum()\n",
    "\n",
    "print(\"\\nConfusion Matrix Details:\")\n",
    "print(f\"True Positives: {true_positives}\")\n",
    "print(f\"False Positives: {false_positives}\")\n",
    "print(f\"True Negatives: {true_negatives}\")\n",
    "print(f\"False Negatives: {false_negatives}\")\n",
    "print(f\"Total Anomaly Timesteps in Ground Truth: {aligned_labels.sum()}\")\n",
    "print(f\"Total Timesteps Flagged as Anomalies: {anomaly_flags.sum()}\")\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_results = {\n",
    "    \"Model\": \"Transformer Autoencoder\",\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1_Score\": f1,\n",
    "    \"True_Positives\": true_positives,\n",
    "    \"False_Positives\": false_positives,\n",
    "    \"True_Negatives\": true_negatives,\n",
    "    \"False_Negatives\": false_negatives,\n",
    "    \"Anomaly_Count_True\": aligned_labels.sum(),\n",
    "    \"Anomaly_Count_Predicted\": anomaly_flags.sum()\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics_results])\n",
    "metrics_df.to_csv(\"transformer_ae_anomaly_detection_metrics.csv\", index=False)\n",
    "print(\"\\nâœ… Metrics saved to 'transformer_ae_anomaly_detection_metrics.csv'.\")\n",
    "\n",
    "# ========================\n",
    "# 9. VISUALIZATION\n",
    "# ========================\n",
    "# Plot 1: Reconstruction Error and Anomalies\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(aggregated_errors, label='Reconstruction Error', alpha=0.7)\n",
    "plt.axhline(threshold, color='r', linestyle='--', label=f'Threshold ({THRESHOLD_PERCENTILE}th percentile)')\n",
    "plt.title('Reconstruction Error with Anomaly Detection')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Detected Anomalies\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(\n",
    "    np.where(anomaly_flags == 1)[0], \n",
    "    aggregated_errors[anomaly_flags == 1], \n",
    "    color='red', \n",
    "    label='Detected Anomalies', \n",
    "    s=20\n",
    ")\n",
    "plt.axhline(threshold, color='r', linestyle='--', label=f'Threshold ({THRESHOLD_PERCENTILE}th percentile)')\n",
    "plt.title('Detected Anomalies')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Comparing with Ground Truth\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(aligned_labels, label='True Anomalies', color='blue', alpha=0.5)\n",
    "plt.plot(anomaly_flags, label='Predicted Anomalies', color='red', alpha=0.5)\n",
    "plt.title('Ground Truth vs Predicted Anomalies')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Anomaly (1=Yes, 0=No)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"transformer_ae_anomaly_detection_results.png\")\n",
    "plt.close()\n",
    "\n",
    "# Additional plot to visualize matches and mismatches\n",
    "plt.figure(figsize=(15, 5))\n",
    "anomaly_comparison = np.zeros(len(aligned_labels))\n",
    "anomaly_comparison[(aligned_labels == 1) & (anomaly_flags == 1)] = 3  # True Positives\n",
    "anomaly_comparison[(aligned_labels == 0) & (anomaly_flags == 1)] = 2  # False Positives\n",
    "anomaly_comparison[(aligned_labels == 1) & (anomaly_flags == 0)] = 1  # False Negatives\n",
    "# True Negatives are left as 0\n",
    "\n",
    "plt.plot(anomaly_comparison, 'o-', markersize=2)\n",
    "plt.grid(True)\n",
    "plt.yticks([0, 1, 2, 3], ['True Negative', 'False Negative', 'False Positive', 'True Positive'])\n",
    "plt.title('Anomaly Detection Performance Visualization')\n",
    "plt.xlabel('Timestep')\n",
    "plt.savefig(\"transformer_ae_detection_performance.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot distribution of reconstruction errors\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(aggregated_errors, bins=100, alpha=0.7)\n",
    "plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold ({THRESHOLD_PERCENTILE}th percentile)')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"transformer_ae_error_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nâœ… All visualizations saved.\")\n",
    "\n",
    "# Optional: Try different threshold values to find optimal F1 score\n",
    "print(\"\\nThreshold Tuning:\")\n",
    "best_f1 = 0\n",
    "best_threshold = threshold\n",
    "best_precision = precision\n",
    "best_recall = recall\n",
    "\n",
    "thresholds = np.percentile(aggregated_errors[counts > 0], [80, 85, 90, 95, 97, 99])\n",
    "for potential_threshold in thresholds:\n",
    "    potential_flags = (aggregated_errors > potential_threshold).astype(int)\n",
    "    potential_precision = precision_score(aligned_labels[:len(potential_flags)], potential_flags, zero_division=0)\n",
    "    potential_recall = recall_score(aligned_labels[:len(potential_flags)], potential_flags, zero_division=0)\n",
    "    potential_f1 = f1_score(aligned_labels[:len(potential_flags)], potential_flags, zero_division=0)\n",
    "    \n",
    "    print(f\"Threshold percentile: {np.round(100 * len(aggregated_errors[aggregated_errors > potential_threshold]) / len(aggregated_errors), 2)}%\")\n",
    "    print(f\"Precision: {potential_precision:.4f}, Recall: {potential_recall:.4f}, F1: {potential_f1:.4f}\")\n",
    "    \n",
    "    if potential_f1 > best_f1:\n",
    "        best_f1 = potential_f1\n",
    "        best_threshold = potential_threshold\n",
    "        best_precision = potential_precision\n",
    "        best_recall = potential_recall\n",
    "\n",
    "print(f\"\\nBest Threshold Found: {best_threshold:.6f}\")\n",
    "print(f\"Best Precision: {best_precision:.4f}\")\n",
    "print(f\"Best Recall: {best_recall:.4f}\")\n",
    "print(f\"Best F1 Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0deb6-8efa-4e6d-896c-a7b61542aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_PERCENTILE = 99.9999\n",
    "\n",
    "# ========================\n",
    "# 9. REAL-TIME SIMULATION ON TEST SET\n",
    "# ========================\n",
    "# Import necessary libraries for ROC curve analysis\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "simulation_X, simulation_y = create_sequences(test_data_tail.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "# Create corresponding labels for evaluation\n",
    "simulation_labels = create_sequence_labels(test_labels_tail.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "\n",
    "forecast_list = []\n",
    "reconstruction_list = []\n",
    "reconstruction_errors = []\n",
    "true_windows = []\n",
    "\n",
    "# Process all samples without overlapping\n",
    "step_size = WINDOW_SIZE_SIMULATION  # Use the window size as step size to avoid overlap\n",
    "for i in range(0, len(simulation_X), step_size):\n",
    "    # Get the current window batch (up to step_size samples)\n",
    "    window_X = simulation_X[i:i+step_size]\n",
    "    window_y_true = simulation_y[i:i+step_size]\n",
    "    \n",
    "    if len(window_X) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Use LSTM Seq2Seq for forecasting\n",
    "    y_pred_future = best_model.predict(window_X, batch_size=128, verbose=1)\n",
    "    X_forecast = y_pred_future  # No need to expand dims as we're processing batches\n",
    "    \n",
    "    # Use Transformer Autoencoder for anomaly detection\n",
    "    y_reconstructed = transformer_ae.predict(X_forecast, batch_size=128, verbose=1)\n",
    "    \n",
    "    # Calculate reconstruction error for each sample in the batch\n",
    "    batch_reconstruction_errors = np.mean((y_pred_future - y_reconstructed)**2, axis=(1, 2))\n",
    "    \n",
    "    # Store predictions and errors\n",
    "    forecast_list.append(y_pred_future)\n",
    "    reconstruction_list.append(y_reconstructed)\n",
    "    reconstruction_errors.append(batch_reconstruction_errors)\n",
    "    true_windows.append(window_y_true)\n",
    "\n",
    "print(\"\\nâœ… Real-time simulation complete.\")\n",
    "\n",
    "# Flatten the reconstruction errors list for threshold calculation\n",
    "all_reconstruction_errors = np.concatenate(reconstruction_errors)\n",
    "\n",
    "# Make sure we're only considering windows where we have both predictions and labels\n",
    "min_length = min(len(simulation_labels), len(all_reconstruction_errors))\n",
    "true_labels_subset = simulation_labels[:min_length]\n",
    "errors_subset = all_reconstruction_errors[:min_length]\n",
    "\n",
    "# Find optimal threshold using ROC curve analysis\n",
    "def find_optimal_threshold_roc(y_true, reconstruction_errors):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold using ROC curve analysis.\n",
    "    Returns the threshold that maximizes Youden's J statistic (TPR - FPR)\n",
    "    \"\"\"\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, reconstruction_errors)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Find the threshold that maximizes TPR - FPR (Youden's J statistic)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, 'b-', label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
    "    plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red', \n",
    "                label=f'Optimal threshold: {optimal_threshold:.5f}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Anomaly Detection')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('roc_curve_threshold.png')\n",
    "    \n",
    "    print(f\"âœ… ROC analysis - Optimal threshold: {optimal_threshold:.5f}\")\n",
    "    print(f\"âœ… ROC analysis - AUC: {roc_auc:.5f}\")\n",
    "    print(f\"âœ… ROC analysis - At optimal threshold: TPR={tpr[optimal_idx]:.5f}, FPR={fpr[optimal_idx]:.5f}\")\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "# For comparison, calculate percentile threshold\n",
    "percentile_threshold = np.percentile(all_reconstruction_errors, THRESHOLD_PERCENTILE)\n",
    "print(f\"âœ… Percentile-based threshold ({THRESHOLD_PERCENTILE}th): {percentile_threshold:.5f}\")\n",
    "\n",
    "# Calculate optimal threshold using ROC curve analysis\n",
    "optimal_threshold = find_optimal_threshold_roc(true_labels_subset, errors_subset)\n",
    "\n",
    "# Plot histogram of reconstruction errors with both thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(all_reconstruction_errors, bins=50, alpha=0.7, density=True)\n",
    "plt.axvline(percentile_threshold, color='red', linestyle='--', \n",
    "            label=f'Percentile Threshold ({THRESHOLD_PERCENTILE}th): {percentile_threshold:.5f}')\n",
    "plt.axvline(optimal_threshold, color='green', linestyle='--', \n",
    "            label=f'Optimal ROC Threshold: {optimal_threshold:.5f}')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Reconstruction Errors with Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('reconstruction_error_histogram.png')\n",
    "\n",
    "# Compare performance of both thresholds\n",
    "percentile_preds = (errors_subset > percentile_threshold).astype(int)\n",
    "optimal_preds = (errors_subset > optimal_threshold).astype(int)\n",
    "\n",
    "percentile_precision = precision_score(true_labels_subset, percentile_preds, zero_division=0)\n",
    "percentile_recall = recall_score(true_labels_subset, percentile_preds, zero_division=0)\n",
    "percentile_f1 = f1_score(true_labels_subset, percentile_preds, zero_division=0)\n",
    "\n",
    "optimal_precision = precision_score(true_labels_subset, optimal_preds, zero_division=0)\n",
    "optimal_recall = recall_score(true_labels_subset, optimal_preds, zero_division=0)\n",
    "optimal_f1 = f1_score(true_labels_subset, optimal_preds, zero_division=0)\n",
    "\n",
    "print(\"\\nðŸ“Š Threshold Method Comparison:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Percentile', 'ROC Optimal'],\n",
    "    'Threshold': [percentile_threshold, optimal_threshold],\n",
    "    'Precision': [percentile_precision, optimal_precision],\n",
    "    'Recall': [percentile_recall, optimal_recall],\n",
    "    'F1 Score': [percentile_f1, optimal_f1]\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "# Use the optimal threshold for anomaly detection\n",
    "threshold = optimal_threshold\n",
    "print(f\"\\nâœ… Using ROC optimal threshold: {threshold:.5f}\")\n",
    "\n",
    "# Apply threshold to get anomaly flags\n",
    "anomaly_flags_list = [errors > threshold for errors in reconstruction_errors]\n",
    "\n",
    "# ========================\n",
    "# 10. EVALUATION\n",
    "# ========================\n",
    "# Forecasting metrics\n",
    "y_pred_all = np.vstack([pred for pred in forecast_list])\n",
    "y_true_all = np.vstack([true for true in true_windows])\n",
    "forecast_rmse = np.sqrt(mean_squared_error(y_true_all.reshape(-1), y_pred_all.reshape(-1)))\n",
    "forecast_mae = mean_absolute_error(y_true_all.reshape(-1), y_pred_all.reshape(-1))\n",
    "print(f\"\\nðŸ“ˆ Forecasting Evaluation on Test:\")\n",
    "print(f\"RMSE: {forecast_rmse:.5f}\")\n",
    "print(f\"MAE:  {forecast_mae:.5f}\")\n",
    "\n",
    "# Anomaly detection metrics - compare both thresholds\n",
    "all_detected = np.concatenate([flags for flags in anomaly_flags_list])\n",
    "\n",
    "# Make sure we're only considering windows where we have both predictions and labels\n",
    "min_length = min(len(simulation_labels), len(all_detected))\n",
    "true_labels_subset = simulation_labels[:min_length]\n",
    "all_detected_subset = all_detected[:min_length]\n",
    "\n",
    "print(f\"\\nAnomaly detection evaluation using ROC optimal threshold:\")\n",
    "print(f\"Array shapes: true_labels={true_labels_subset.shape}, detected={all_detected_subset.shape}\")\n",
    "print(f\"Number of true anomalies: {np.sum(true_labels_subset)}\")\n",
    "print(f\"Number of detected anomalies: {np.sum(all_detected_subset)}\")\n",
    "\n",
    "# Ensure we have compatible arrays for evaluation metrics\n",
    "if true_labels_subset.shape != all_detected_subset.shape:\n",
    "    print(f\"âš ï¸ Warning: Array shapes don't match. Truncating to minimum length.\")\n",
    "    min_len = min(len(true_labels_subset), len(all_detected_subset))\n",
    "    true_labels_subset = true_labels_subset[:min_len]\n",
    "    all_detected_subset = all_detected_subset[:min_len]\n",
    "\n",
    "# Compute metrics using true labels and ROC optimal threshold\n",
    "precision = precision_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "recall = recall_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "f1 = f1_score(true_labels_subset, all_detected_subset, zero_division=0)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Anomaly Detection Evaluation (ROC Optimal Threshold):\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall:    {recall:.5f}\")\n",
    "print(f\"F1 Score:  {f1:.5f}\")\n",
    "\n",
    "# ========================\n",
    "# 11. SAVE METRICS\n",
    "# ========================\n",
    "metrics_results = {\n",
    "    \"Model\": \"LSTM Seq2Seq + Transformer AE\",\n",
    "    \"Threshold_Method\": \"ROC Optimal\",\n",
    "    \"Threshold_Value\": threshold,\n",
    "    \"Forecast_RMSE\": forecast_rmse,\n",
    "    \"Forecast_MAE\": forecast_mae,\n",
    "    \"Anomaly_Precision\": precision,\n",
    "    \"Anomaly_Recall\": recall,\n",
    "    \"Anomaly_F1\": f1\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics_results])\n",
    "metrics_df.to_csv(\"metrics_lstm_seq2seq_transformer_ae_pipeline.csv\", index=False)\n",
    "print(\"\\nâœ… Metrics saved to 'metrics_lstm_seq2seq_transformer_ae_pipeline.csv'.\")\n",
    "\n",
    "# ========================\n",
    "# 12. PLOTS\n",
    "# ========================\n",
    "# Plot Reconstruction Errors with True Labels\n",
    "plt.figure(figsize=(14,5))\n",
    "all_errors = all_reconstruction_errors\n",
    "\n",
    "# Ensure we're only plotting up to the minimum length we have data for\n",
    "min_plot_len = min(len(all_errors), len(true_labels_subset))\n",
    "plot_errors = all_errors[:min_plot_len]\n",
    "plot_labels = true_labels_subset[:min_plot_len]\n",
    "\n",
    "plt.plot(plot_errors, label='Reconstruction Error')\n",
    "plt.axhline(threshold, color='red', linestyle='--', label=f'ROC Optimal Threshold: {threshold:.5f}')\n",
    "\n",
    "# Detected anomalies\n",
    "detected_indices = np.where(optimal_preds[:min_plot_len] == 1)[0]\n",
    "plt.scatter(detected_indices,\n",
    "            plot_errors[detected_indices],\n",
    "            color='red', label='Detected Anomalies', s=10)\n",
    "\n",
    "# True anomalies\n",
    "true_anomaly_indices = np.where(plot_labels == 1)[0]\n",
    "if len(true_anomaly_indices) > 0:\n",
    "    plt.scatter(true_anomaly_indices,\n",
    "                np.ones_like(true_anomaly_indices) * np.max(plot_errors)*0.9,\n",
    "                color='green', marker='*', label='True Anomalies', s=20)\n",
    "else:\n",
    "    print(\"No true anomalies found in the subset of data being visualized\")\n",
    "\n",
    "plt.title(\"Reconstruction Errors vs True Anomalies (ROC Optimal Threshold)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reconstruction_errors_lstm_transformer_pipeline.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
