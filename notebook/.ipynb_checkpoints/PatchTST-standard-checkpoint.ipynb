{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741771f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 0. IMPORTS\n",
    "# ========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, LSTM, RepeatVector, TimeDistributed, \n",
    "                                     MultiHeadAttention, LayerNormalization, Add, \n",
    "                                     Conv1D, GlobalAveragePooling1D)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from einops import rearrange\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20eae751-ff6d-42b2-85a8-77cad5c32c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. CONFIGURATION\n",
    "# ========================\n",
    "INPUT_STEPS = 20\n",
    "FORECAST_STEPS = 20\n",
    "# PATCH_LEN = 3   # must divide INPUT_STEPS\n",
    "TEST_RATIO = 0.3\n",
    "\n",
    "# Tuning parameters\n",
    "EPOCHS_LIST = [10, 20]\n",
    "BATCH_SIZES = [64, 128]\n",
    "\n",
    "# Simulation parameters\n",
    "WINDOW_SIZE_SIMULATION = 12  # 6 hours ‚Üí 6√ó6=36 steps (if 10-min data)\n",
    "THRESHOLD_PERCENTILE = 95\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0495116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU is available and will be used.\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 2. DEVICE SETUP\n",
    "# ========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(\"‚úÖ GPU is available and will be used.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe805735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scaled dataset shape: (62174, 26)\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 3. LOAD AND PREPROCESS DATA\n",
    "# ========================\n",
    "file_path = '../rfcc_longest_active_window.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'], errors='coerce')\n",
    "df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Clean and normalize\n",
    "df.dropna(axis=1, thresh=int(0.7 * len(df)), inplace=True)\n",
    "df.ffill(inplace=True)\n",
    "df.bfill(inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df.values)\n",
    "df_scaled = pd.DataFrame(scaled, index=df.index, columns=df.columns).astype(np.float32)\n",
    "\n",
    "print(f\"‚úÖ Scaled dataset shape: {df_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a2eb62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training samples: 43521, Testing samples: 18653\n",
      "‚úÖ Training sequences: (43481, 20, 26), Testing sequences: (18613, 20, 26)\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 4. SEQUENTIAL TRAIN/TEST SPLIT\n",
    "# ========================\n",
    "split_idx = int((1 - TEST_RATIO) * len(df_scaled))\n",
    "train_data = df_scaled.iloc[:split_idx]\n",
    "test_data = df_scaled.iloc[split_idx:]\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_data)}, Testing samples: {len(test_data)}\")\n",
    "\n",
    "# ========================\n",
    "# 5. PATCH SEQUENCE GENERATOR\n",
    "# ========================\n",
    "def create_patch_sequences(data, input_steps, forecast_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_steps - forecast_steps):\n",
    "        X.append(data[i:i+input_steps])\n",
    "        y.append(data[i+input_steps:i+input_steps+forecast_steps])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = create_patch_sequences(train_data.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "X_test, y_test = create_patch_sequences(test_data.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "\n",
    "print(f\"‚úÖ Training sequences: {X_train.shape}, Testing sequences: {X_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# # ========================\n",
    "# # 4. COMBINE TRAIN + TEST SETS\n",
    "# # ========================\n",
    "# # Au lieu de splitter, on prend toutes les donn√©es comme train_set\n",
    "# full_train_data = df_scaled  # On utilise tout le dataset\n",
    "\n",
    "# # On cr√©e quand m√™me un petit validation_set (10%) s√©quentiel\n",
    "# val_ratio = 0.1\n",
    "# split_idx = int((1 - val_ratio) * len(full_train_data))\n",
    "# train_data = full_train_data.iloc[:split_idx]\n",
    "# val_data = full_train_data.iloc[split_idx:]\n",
    "\n",
    "# print(f\"‚úÖ Full training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "\n",
    "# # ========================\n",
    "# # 5. PATCH SEQUENCE GENERATOR (identique)\n",
    "# # ========================\n",
    "# def create_patch_sequences(data, input_steps, forecast_steps):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(data) - input_steps - forecast_steps):\n",
    "#         X.append(data[i:i+input_steps])\n",
    "#         y.append(data[i+input_steps:i+input_steps+forecast_steps])\n",
    "#     return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "# X_train, y_train = create_patch_sequences(train_data.values, INPUT_STEPS, FORECAST_STEPS)\n",
    "# X_val, y_val = create_patch_sequences(val_data.values, INPUT_STEPS, FORECAST_STEPS)  # Remplace X_test/y_test\n",
    "\n",
    "# print(f\"‚úÖ Training sequences: {X_train.shape}, Validation sequences: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "722bcb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 6. BUILD PATCHTST MODEL (CORRECTED)\n",
    "# ========================\n",
    "def build_patchtst(input_steps, num_features, patch_len=4, embed_dim=128, num_heads=4, num_layers=1):\n",
    "    assert input_steps % patch_len == 0, f\"Input steps ({input_steps}) must be divisible by patch length ({patch_len})\"\n",
    "    num_patches = input_steps // patch_len\n",
    "\n",
    "    inp = Input(shape=(input_steps, num_features))\n",
    "    \n",
    "    # Channel Independence\n",
    "    x = tf.reshape(inp, (-1, input_steps, 1))  # (batch*num_features, timesteps, 1)\n",
    "    \n",
    "    # Patching\n",
    "    x = tf.reshape(x, (-1, num_patches, patch_len))  # (batch*num_features, num_patches, patch_len)\n",
    "    x = Dense(embed_dim)(x)  # (batch*num_features, num_patches, embed_dim)\n",
    "\n",
    "    # Transformer Blocks\n",
    "    for _ in range(num_layers):\n",
    "        # Self-Attention\n",
    "        attn = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "        x = LayerNormalization()(Add()([x, attn]))\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        ffn = Dense(embed_dim * 4, activation='gelu')(x)\n",
    "        ffn = Dense(embed_dim)(ffn)\n",
    "        x = LayerNormalization()(Add()([x, ffn]))\n",
    "\n",
    "    # Output Head\n",
    "    x = x[:, -1, :]  # Last patch only (batch*num_features, embed_dim)\n",
    "    x = tf.reshape(x, (-1, num_features, embed_dim))  # (batch, num_features, embed_dim)\n",
    "    \n",
    "    # Ensure output matches FORECAST_STEPS\n",
    "    x = Dense(FORECAST_STEPS)(x)  # (batch, num_features, forecast_steps)\n",
    "    out = tf.transpose(x, [0, 2, 1])  # (batch, forecast_steps, num_features)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37d072c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (43481, 20, 26), y_train shape: (43481, 20, 26)\n",
      "Input steps: 20, Forecast steps: 20, Features: 26\n",
      "1/1 [==============================] - 0s 418ms/step\n",
      "Test prediction shape: (1, 20, 26) (should match (1, 20, 26))\n",
      "\n",
      "üîµ Training with epochs=10, batch_size=64\n",
      "Epoch 1/10\n",
      "612/612 [==============================] - 15s 22ms/step - loss: 0.0560 - val_loss: 0.0411\n",
      "Epoch 2/10\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.0430 - val_loss: 0.0424\n",
      "Epoch 3/10\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.0421 - val_loss: 0.0365\n",
      "Epoch 4/10\n",
      "612/612 [==============================] - 14s 22ms/step - loss: 0.0415 - val_loss: 0.0367\n",
      "Epoch 5/10\n",
      "612/612 [==============================] - 13s 22ms/step - loss: 0.0410 - val_loss: 0.0367\n",
      "Epoch 6/10\n",
      "612/612 [==============================] - 14s 22ms/step - loss: 0.0410 - val_loss: 0.0356\n",
      "Epoch 7/10\n",
      "612/612 [==============================] - 13s 22ms/step - loss: 0.0410 - val_loss: 0.0349\n",
      "Epoch 8/10\n",
      "612/612 [==============================] - 13s 22ms/step - loss: 0.0415 - val_loss: 0.0357\n",
      "Epoch 9/10\n",
      "612/612 [==============================] - 15s 25ms/step - loss: 0.0406 - val_loss: 0.0354\n",
      "Epoch 10/10\n",
      "612/612 [==============================] - 16s 26ms/step - loss: 0.0404 - val_loss: 0.0344\n",
      "291/291 [==============================] - 3s 11ms/step\n",
      "‚úÖ Val MSE: 0.06544, MAE: 0.21313\n",
      "üèÜ New best model!\n",
      "\n",
      "üîµ Training with epochs=10, batch_size=128\n",
      "Epoch 1/10\n",
      "306/306 [==============================] - 14s 43ms/step - loss: 0.0651 - val_loss: 0.0393\n",
      "Epoch 2/10\n",
      "306/306 [==============================] - 13s 43ms/step - loss: 0.0463 - val_loss: 0.0379\n",
      "Epoch 3/10\n",
      "306/306 [==============================] - 13s 43ms/step - loss: 0.0457 - val_loss: 0.0381\n",
      "Epoch 4/10\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0456 - val_loss: 0.0370\n",
      "Epoch 5/10\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0444 - val_loss: 0.0357\n",
      "Epoch 6/10\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0430 - val_loss: 0.0351\n",
      "Epoch 7/10\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0423 - val_loss: 0.0348\n",
      "Epoch 8/10\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0422 - val_loss: 0.0347\n",
      "Epoch 9/10\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0417 - val_loss: 0.0351\n",
      "Epoch 10/10\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0413 - val_loss: 0.0349\n",
      "146/146 [==============================] - 3s 17ms/step\n",
      "‚úÖ Val MSE: 0.06190, MAE: 0.20234\n",
      "üèÜ New best model!\n",
      "\n",
      "üîµ Training with epochs=20, batch_size=64\n",
      "Epoch 1/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.0621 - val_loss: 0.0430\n",
      "Epoch 2/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.0465 - val_loss: 0.0396\n",
      "Epoch 3/20\n",
      "612/612 [==============================] - 16s 26ms/step - loss: 0.0433 - val_loss: 0.0360\n",
      "Epoch 4/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.0420 - val_loss: 0.0365\n",
      "Epoch 5/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.0413 - val_loss: 0.0365\n",
      "Epoch 6/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.0405 - val_loss: 0.0363\n",
      "Epoch 7/20\n",
      "612/612 [==============================] - 16s 26ms/step - loss: 0.0403 - val_loss: 0.0372\n",
      "Epoch 8/20\n",
      "612/612 [==============================] - 15s 25ms/step - loss: 0.0402 - val_loss: 0.0357\n",
      "Epoch 9/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.0405 - val_loss: 0.0354\n",
      "Epoch 10/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.0405 - val_loss: 0.0360\n",
      "Epoch 11/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.0400 - val_loss: 0.0354\n",
      "Epoch 12/20\n",
      "612/612 [==============================] - 16s 26ms/step - loss: 0.0396 - val_loss: 0.0351\n",
      "Epoch 13/20\n",
      "612/612 [==============================] - 17s 28ms/step - loss: 0.0395 - val_loss: 0.0354\n",
      "Epoch 14/20\n",
      "612/612 [==============================] - 17s 28ms/step - loss: 0.0393 - val_loss: 0.0356\n",
      "Epoch 15/20\n",
      "612/612 [==============================] - 17s 28ms/step - loss: 0.0394 - val_loss: 0.0355\n",
      "Epoch 16/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.0397 - val_loss: 0.0356\n",
      "Epoch 17/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.0394 - val_loss: 0.0359\n",
      "291/291 [==============================] - 3s 9ms/step\n",
      "‚úÖ Val MSE: 0.07348, MAE: 0.22197\n",
      "\n",
      "üîµ Training with epochs=20, batch_size=128\n",
      "Epoch 1/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0678 - val_loss: 0.0387\n",
      "Epoch 2/20\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0459 - val_loss: 0.0372\n",
      "Epoch 3/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0441 - val_loss: 0.0361\n",
      "Epoch 4/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0430 - val_loss: 0.0357\n",
      "Epoch 5/20\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0424 - val_loss: 0.0355\n",
      "Epoch 6/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0420 - val_loss: 0.0357\n",
      "Epoch 7/20\n",
      "306/306 [==============================] - 14s 45ms/step - loss: 0.0414 - val_loss: 0.0353\n",
      "Epoch 8/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0411 - val_loss: 0.0355\n",
      "Epoch 9/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0409 - val_loss: 0.0355\n",
      "Epoch 10/20\n",
      "306/306 [==============================] - 13s 44ms/step - loss: 0.0407 - val_loss: 0.0354\n",
      "Epoch 11/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0405 - val_loss: 0.0353\n",
      "Epoch 12/20\n",
      "306/306 [==============================] - 14s 45ms/step - loss: 0.0402 - val_loss: 0.0349\n",
      "Epoch 13/20\n",
      "306/306 [==============================] - 14s 45ms/step - loss: 0.0402 - val_loss: 0.0351\n",
      "Epoch 14/20\n",
      "306/306 [==============================] - 14s 45ms/step - loss: 0.0403 - val_loss: 0.0352\n",
      "Epoch 15/20\n",
      "306/306 [==============================] - 14s 45ms/step - loss: 0.0402 - val_loss: 0.0352\n",
      "Epoch 16/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0402 - val_loss: 0.0353\n",
      "Epoch 17/20\n",
      "306/306 [==============================] - 14s 44ms/step - loss: 0.0401 - val_loss: 0.0349\n",
      "146/146 [==============================] - 3s 16ms/step\n",
      "‚úÖ Val MSE: 0.07220, MAE: 0.21518\n",
      "\n",
      "üìã Tuning Results:\n",
      "   epochs  batch_size   val_mse   val_mae  best_epoch\n",
      "1      10         128  0.061903  0.202343           8\n",
      "0      10          64  0.065437  0.213129          10\n",
      "3      20         128  0.072199  0.215180          12\n",
      "2      20          64  0.073477  0.221972          12\n",
      "\n",
      "‚úÖ Best model saved with:\n",
      "- Val MSE: 0.06190\n",
      "- Best config: epochs         10.000000\n",
      "batch_size    128.000000\n",
      "val_mse         0.061903\n",
      "val_mae         0.202343\n",
      "best_epoch      8.000000\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 7. TRAINING + TUNING (CORRECTED)\n",
    "# ========================\n",
    "# First verify shapes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"Input steps: {INPUT_STEPS}, Forecast steps: {FORECAST_STEPS}, Features: {X_train.shape[2]}\")\n",
    "\n",
    "# Test model output shape\n",
    "test_model = build_patchtst(INPUT_STEPS, X_train.shape[2])\n",
    "test_pred = test_model.predict(X_train[:1])\n",
    "print(f\"Test prediction shape: {test_pred.shape} (should match {y_train[:1].shape})\")\n",
    "\n",
    "assert test_pred.shape == y_train[:1].shape, \\\n",
    "    f\"Shape mismatch! Model outputs {test_pred.shape} but y_train has {y_train[:1].shape}\"\n",
    "\n",
    "best_val_mse = np.inf\n",
    "best_model = None\n",
    "history_records = []\n",
    "\n",
    "for epochs in EPOCHS_LIST:\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\nüîµ Training with epochs={epochs}, batch_size={batch_size}\")\n",
    "        \n",
    "        model = build_patchtst(INPUT_STEPS, X_train.shape[2])\n",
    "        es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es],\n",
    "            verbose=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        val_preds = model.predict(X_test, batch_size=batch_size)\n",
    "        \n",
    "        # Flatten all except batch dimension for metrics\n",
    "        val_mse = mean_squared_error(\n",
    "            y_test.reshape(-1, FORECAST_STEPS * X_test.shape[2]), \n",
    "            val_preds.reshape(-1, FORECAST_STEPS * X_test.shape[2])\n",
    "        )\n",
    "        val_mae = mean_absolute_error(\n",
    "            y_test.reshape(-1, FORECAST_STEPS * X_test.shape[2]),\n",
    "            val_preds.reshape(-1, FORECAST_STEPS * X_test.shape[2])\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Val MSE: {val_mse:.5f}, MAE: {val_mae:.5f}\")\n",
    "\n",
    "        history_records.append({\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"val_mse\": val_mse,\n",
    "            \"val_mae\": val_mae,\n",
    "            \"best_epoch\": np.argmin(history.history['val_loss']) + 1\n",
    "        })\n",
    "\n",
    "        if val_mse < best_val_mse:\n",
    "            best_val_mse = val_mse\n",
    "            best_model = model\n",
    "            print(\"üèÜ New best model!\")\n",
    "\n",
    "# Save results\n",
    "history_df = pd.DataFrame(history_records)\n",
    "history_df.to_csv(\"patchtst_tuning_history.csv\", index=False)\n",
    "print(\"\\nüìã Tuning Results:\")\n",
    "print(history_df.sort_values('val_mse'))\n",
    "\n",
    "if best_model:\n",
    "    best_model.save(\"best_patchtst_model.h5\")\n",
    "    print(\"\\n‚úÖ Best model saved with:\")\n",
    "    print(f\"- Val MSE: {best_val_mse:.5f}\")\n",
    "    print(f\"- Best config: {history_df.loc[history_df['val_mse'].idxmin()]}\")\n",
    "\n",
    "\n",
    "# # ========================\n",
    "# # 7. TRAINING + TUNING (CORRECTED)\n",
    "# # ========================\n",
    "# # First verify shapes\n",
    "# print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "# print(f\"Input steps: {INPUT_STEPS}, Forecast steps: {FORECAST_STEPS}, Features: {X_train.shape[2]}\")\n",
    "\n",
    "# # Test model output shape\n",
    "# test_model = build_patchtst(INPUT_STEPS, X_train.shape[2])\n",
    "# test_pred = test_model.predict(X_train[:1])\n",
    "# print(f\"Test prediction shape: {test_pred.shape} (should match {y_train[:1].shape})\")\n",
    "\n",
    "# assert test_pred.shape == y_train[:1].shape, \\\n",
    "#     f\"Shape mismatch! Model outputs {test_pred.shape} but y_train has {y_train[:1].shape}\"\n",
    "\n",
    "# best_val_mse = np.inf\n",
    "# best_model = None\n",
    "# history_records = []\n",
    "\n",
    "# for epochs in EPOCHS_LIST:\n",
    "#     for batch_size in BATCH_SIZES:\n",
    "#         print(f\"\\nüîµ Training with epochs={epochs}, batch_size={batch_size}\")\n",
    "        \n",
    "#         model = build_patchtst(INPUT_STEPS, X_train.shape[2])\n",
    "#         es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "#         history = model.fit(\n",
    "#             X_train, y_train,\n",
    "#             validation_split=0.1,\n",
    "#             epochs=epochs,\n",
    "#             batch_size=batch_size,\n",
    "#             callbacks=[es],\n",
    "#             verbose=1,\n",
    "#             shuffle=False\n",
    "#         )\n",
    "        \n",
    "#         val_preds = model.predict(X_val, batch_size=batch_size)\n",
    "        \n",
    "#         # Flatten all except batch dimension for metrics\n",
    "#         val_mse = mean_squared_error(\n",
    "#             y_val.reshape(-1, FORECAST_STEPS * X_val.shape[2]),  # <-- y_val au lieu de y_test\n",
    "#             val_preds.reshape(-1, FORECAST_STEPS * X_val.shape[2])\n",
    "#         )\n",
    "#         val_mae = mean_absolute_error(\n",
    "#             y_test.reshape(-1, FORECAST_STEPS * X_test.shape[2]),\n",
    "#             val_preds.reshape(-1, FORECAST_STEPS * X_test.shape[2])\n",
    "#         )\n",
    "\n",
    "#         print(f\"‚úÖ Val MSE: {val_mse:.5f}, MAE: {val_mae:.5f}\")\n",
    "\n",
    "#         history_records.append({\n",
    "#             \"epochs\": epochs,\n",
    "#             \"batch_size\": batch_size,\n",
    "#             \"val_mse\": val_mse,\n",
    "#             \"val_mae\": val_mae,\n",
    "#             \"best_epoch\": np.argmin(history.history['val_loss']) + 1\n",
    "#         })\n",
    "\n",
    "#         if val_mse < best_val_mse:\n",
    "#             best_val_mse = val_mse\n",
    "#             best_model = model\n",
    "#             print(\"üèÜ New best model!\")\n",
    "\n",
    "# # Save results\n",
    "# history_df = pd.DataFrame(history_records)\n",
    "# history_df.to_csv(\"patchtst_tuning_history.csv\", index=False)\n",
    "# print(\"\\nüìã Tuning Results:\")\n",
    "# print(history_df.sort_values('val_mse'))\n",
    "\n",
    "# if best_model:\n",
    "#     best_model.save(\"best_patchtst_model.h5\")\n",
    "#     print(\"\\n‚úÖ Best model saved with:\")\n",
    "#     print(f\"- Val MSE: {best_val_mse:.5f}\")\n",
    "#     print(f\"- Best config: {history_df.loc[history_df['val_mse'].idxmin()]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
